{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KI Implementierung (othello_ai.ipynb)\n",
    "\\label{sec:aiimpl}\n",
    "\\ifx false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".container { width:100% }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\fi\n",
    "Dieses Kapitel beschreibt die Implementierung der \\ac{KI}. Es werden mehrere unterschiedliche Strategien implementiert, um einen Vergleich von diesen zu ermöglichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importieren der externen Abhängigkeiten\n",
    "Zur Initialisierung der Parameter `alpha` und `beta` in der mit Alpha-Beta Pruning optimierten Minimax Strategie werden die Konstanten `math.inf` und `-math.inf` benötigt. Sie stehen jeweils für den maximalen und den minimalen Wert, den eine Fließkommazahl annehmen kann. Die Konstanten werden von der Python Standardbibliothek in dem Modul `math` bereitgestellt\n",
    "\n",
    "Das Modul `random` wird im Rahmen dieser Implementierung für mehrere Zwecke genutzt.\n",
    "Zum einen zur Implementierung der `random_ai`, einer Strategie, welche immer einen zufälligen Zug wählt und zum anderen, um die auf Minimax basierenden Strategien nicht-deterministisch zu machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Implementierung der \\ac{KI} baut auf der Implementierung der Spielelogik von Othello auf. Das entsprechende Notebook muss also hier ausgeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run othello_game.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die aktuellen Nutzen-Werte der beiden Spieler werden in der globalen Variable utilities gespeichert, sodass diese in der GUI angezeigt werden können. Diese Werte werden in den entsprechenden Funktionen der Strategien aktualisiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilities = {WHITE: '-', BLACK: '-'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristiken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Abschätzen der Nützlichkeit eines Spielzustands wird eine Heuristik benötigt. Im Folgenden sind einige solcher Heuristiken implementiert. Da Weiß der maximierende Spieler und Schwarz der minimierende Spieler ist, repräsentiert ein höherer Wert der Heuristik einen für Weiß vorteilhaften Zug, während ein niedriger Wert einen Vorteil für Schwarz repräsentiert. Die Werte aller Heuristiken liegen zwischen $-1$ und $1$, wobei diese Randwerte einen garantierten Sieg für den jeweiligen Spieler darstellen. Der Wert $0$ steht für einen für beide Spieler gleich guten Spielzustand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `disc_count_heuristic` implementiert die in \\ref{sec:disccount} beschriebene Heuristik. Dafür wird die Differenz der Anzahlen an Steinen beider Spieler auf dem Spielfeld bestimmt. Diese wird zur Normalisierung durch die maximale Steinzahl $64$ geteilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_count_heuristic(state):\n",
    "    return (count_disks(state, WHITE) - count_disks(state, BLACK)) / 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mobility_heuristic` berechnet wie in \\ref{sec:theorycurrentmobility} beschrieben die aktuelle Mobilität. Auch dieser Wert wird durch Division durch die Anzahl an Feldern normalisiert, um die Grenzen von $-1$ und $1$ einzuhalten. Zu beachten ist hier, dass auch die Anzahl möglicher Züge für einen Spieler bestimmt wird, der im Spielzustand gar nicht am Zug ist. Dies wirkt zunächst sematisch nicht sinvoll, hat sich jedoch, wie in \\ref{sec:currentmobility} gezeigt, im Vergleich gegenüber möglicher Alternativen, wie der Verwendung einer durchschnittlichen Mobilität, als effektiver erwiesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobility_heuristic(state):\n",
    "    if state.turn == WHITE:\n",
    "        return (len(state.possible_moves) -\n",
    "                len(get_possible_moves(state, BLACK))) / 64\n",
    "    else:\n",
    "        return (len(get_possible_moves(state, WHITE)) -\n",
    "                len(state.possible_moves)) / 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicht nur die aktuelle, sondern auch die potenzielle Mobilität, welche in \\ref{sec:potmobility} beschrieben wird, kann vor allem in frühen Phasen des Spiels wichtig für die Bewertung einer Position sein. Die Funktion `pot_mob_heuristic` berechnet für einen Zustand `state` die Differenz der potenziellen Mobilität beider Spieler. Die potenzielle Mobilität eines Spielers ist hierbei gegeben durch die Summe aller freier Felder um gegnerische Spielsteine, da Michael Buro dieses Merkmal in seiner Dissertation als beste Metrik für die potenzielle Mobilität ausgemacht hat \\cite[S. 9]{evaluationfunctions}. Das Ergebnis wird durch $3.5$ geteilt, da es in zufälligen Spielen im Durchschnitt $3.5$ mal so viele potenzielle Züge wie tatsächliche Züge gibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pot_mob_heuristic(state):\n",
    "    board = list(state.board)\n",
    "    fields = 0\n",
    "    for (x,y) in state.frontier:\n",
    "        for dx, dy in directions:\n",
    "            xi = x + dx\n",
    "            yi = y + dy\n",
    "            if 0 <= xi < 8 and 0 <= yi < 8 and board[xi][yi] != 0:\n",
    "                fields -= board[xi][yi]\n",
    "    fields /= 3.5 \n",
    "    return fields / 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `combined_mobility_heuristic` kombiniert die aktuelle und potenzielle Mobilität, wobei zu Beginn des Spiels die potenzielle Mobilität stärker gewichtet wird und gegen Ende des Spiels die aktuelle Mobilität. Michael Buro beschreibt in seiner Dissertation, dass die potenzielle Mobilität, bis 36 Spielsteine auf dem Feld liegen, wichtiger für die Bewertung ist, als die aktuelle Mobilität \\cite[S. 9]{evaluationfunctions}. Eigene Tests, welche in Kapitel \\ref{sec:combinedmobility} beschrieben sind, ergeben, dass eine lineare Kombination der beiden Merkmale zu einem guten Ergebnis führt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_mobility_heuristic(state):\n",
    "    act = mobility_heuristic(state)\n",
    "    pot = pot_mob_heuristic(state)\n",
    "    return (1 - state.num_pieces / 50) * pot + (state.num_pieces / 50) *  act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beim Spielen von Othello fällt auf, dass es bestimmte Felder gibt, deren Belegung von Vorteil ist, sowie einige, deren Belegung eher nachteilhaft ist. Diese Eigenschaft macht sich die Cowthello-Heuristik \\cite{cowthello} zunutze, welche in \\ref{sec:cowthello} beschrieben ist. Sie weist jedem Feld einen Wert zu, der angibt, wie Vorteilhaft der Besitz dieses Feldes ist, bzw. wie Nachteilhaft die Belegung des Feldes durch den Gegner ist. Diese Gewichte werden mit der aktuellen Belegung des Spielfelds multipliziert und die Ergebnisse anschließend aufsummiert. Der resultierende Wert schätzt dann den Nutzen der aktuellen Position ein. Auch bei dieser Heuristik findet eine Normalisierung statt.\n",
    "\n",
    "Da die Ecken des Spielfeldes, sobald sie einmal belegt sind, nicht mehr vom Gegner erobert werden können, spielen diese eine besondere Rolle. Ist eine Ecke unbelegt, so sind die umliegenden Spielfelder anders zu bewerten, als bei einer belegten Ecke. Dies wird von der Cowthello Heuristik berücksichtigt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgrund der Symmetrie des Othello Spielfeldes, ist es für die Cowthello-Heuristik nicht nötig, für jedes Feld einzeln dessen Gewicht anzugeben. Stattdessen werden ausschließlich die Gewichte für ein Viertel des Spielfelds angegeben und dieses anschließend gespiegelt. Die Funktion `gen_cowthello_matrix` generiert dann die Gewichte-Matrix für das gesamte Feld und führt auch die Normalisierung durch. Dabei werden die Gewichte aus dem Online-Othello Programm Cowthello \\cite{cowthello} verwendet. Cowthello ist unter der URL <https://www.aurochs.org/games/cowthello/> verfügbar. Die Funktion `gen_cowthello_matrix` erzeugt hierbei die bei unbelegten Ecken verwendeten Gewichte. Sind Ecken belegt, werden die Gewichte später entsprechend angepasst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_cowthello_matrix():\n",
    "    quarter = np.array([\n",
    "        [100, -25, 25, 10],\n",
    "        [-25, -50,  1,  1],\n",
    "        [ 25,   1, 50,  5],\n",
    "        [ 10,   1,  5,  1]\n",
    "    ])\n",
    "    top = np.hstack((quarter, np.flip(quarter, axis=1)))\n",
    "    bottom = np.flip(top, axis=0)\n",
    "    cowthello_matrix = np.vstack((top, bottom))\n",
    "    return cowthello_matrix;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `modify_corner_weights` übernimmt die Modifikation der Gewichte-Matrix, für eine belegte Ecke. Die Ecke wird hierbei durch deren Koordinaten `row` und `column`, sowie die Richtung zum Mittelfeld des Spielfels `rdir`, `cdir` spezifiziert. Die als Parameter übergebene Gewichte-Matrix `weights` wird hierbei verändert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_corner_weights(weights, row, col, rdir, cdir):\n",
    "    weights[row+rdir   , col       ] = 10\n",
    "    weights[row+2*rdir , col       ] = 10\n",
    "    weights[row        , col+cdir  ] = 10\n",
    "    weights[row        , col+2*cdir] = 10\n",
    "    weights[row+rdir   , col+cdir  ] = 1\n",
    "    weights[row+2*rdir , col+2*cdir] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um den Wert der Cowthello Heuristik zwischen 0 und 1 einzugrenzen, wird der maximal mögliche Wert der Heuristik berechnet und in der Variable `norm_factor` gepeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_factor = np.sum(np.abs(gen_cowthello_matrix()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `cowthello_heuristic` bestimmt aus einem Spielzustand und der aufgestellten Gewichtematrix `cowthello_weights` die gewichtete Summe, welche als Heuristik genutzt wird. Für jede belegte Ecke wird die Gewichte-Matrix mittels `modify_corner_weights` entsprechend modifiziert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cowthello_heuristic(state):\n",
    "    weights = gen_cowthello_matrix()\n",
    "    if state.board[0, 0] != NONE:\n",
    "        modify_corner_weights(weights, 0, 0, 1, 1)\n",
    "    if state.board[7, 0] != NONE:\n",
    "        modify_corner_weights(weights, 7, 0,-1, 1)\n",
    "    if state.board[0, 7] != NONE:\n",
    "        modify_corner_weights(weights, 0, 7, 1,-1)\n",
    "    if state.board[7, 7] != NONE:\n",
    "        modify_corner_weights(weights, 7, 7,-1,-1)\n",
    "    heuristic = np.sum(np.multiply(state.board,weights))\n",
    "    return heuristic / norm_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die oben implementierten Heuristiken bewerten jeweils nur ein Merkmal der aktuellen Spielsitation. Durch eine Kombination mehrerer dieser Heuristiken können mehrere Merkmale gleichzeitig betrachtet werden. Die Gewichtung von Mobilität und Cowthello wird in Kapitel \\ref{sec:mobcowweight} bestimmt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_heuristic(state):\n",
    "    mobility = combined_mobility_heuristic(state)\n",
    "    cowthello = cowthello_heuristic(state)\n",
    "    return 0.625 * mobility + 0.375 * cowthello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung der Strategien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden werden die verschiedenen Strategien der \\ac{KI} implementiert. Diese verwenden zum Teil die im vorherigen Kaptitel implementierten Heuristiken.\n",
    "Die Strategien bestehen jeweils aus einer bewertenden Funktion, die den Nutzen einer Spielsituation bestimmt, sowie aus einer aufrufenden Funktion, welche mithilfe der bewertenden Funktion den bestmöglichen Zug ermittelt. Diese Komponenten können beliebig kombiniert werden. Im Folgenden werden zunächst die bewertenden Funktionen implementiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zufällige KI\n",
    "Die zufällige \\ac{KI} bewertet den Nutzen aller Züge gleich, gibt also immer den Wert $0$ zurück.  Da die Strategie-Funktionen völlig austauschbar sein sollen, müssen alle diese Funktionen dieselben Eingabeparameter haben. Im Fall der Funktion `random_ai` wird jedoch keiner der definierten Parameter benötigt. Der Zweck dieser \\ac{KI} ist die Messung der Stärke der anderen \\acp{KI}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_ai(state, depth, heuristic, alpha, beta):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimax KI\n",
    "Die Minimax Strategie verwendet den unveränderten Minimax Algorithmus, wie er in \\autoref{sec:minimax} beschrieben ist, zur Bestimmung der Nützlichkeit eines Zuges. Eingabeparameter sind hier der zu bewertende Spielzustand `state`, die gewünschte Suchtiefe `depth` sowie die zu verwendende Heuristik\n",
    "`heuristic`. Die Parameter `alpha` und `beta` dienen, wie oben beschrieben, der Kompatibilität mit den folgenden Strategie-Funktionen und werden in der Funktion `minimax` nicht verwendet. Der Rückgabeparameter gibt die ermittelte Nützlichkeit des Spielzustands an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_mm_count = 0\n",
    "\n",
    "def minimax(state, depth, heuristic, alpha, beta):\n",
    "    global debug_mm_count\n",
    "    if state.game_over:\n",
    "        return get_utility(state)\n",
    "    if depth == 0:\n",
    "        debug_mm_count += 1\n",
    "        return heuristic(state)\n",
    "\n",
    "    if state.turn == WHITE:\n",
    "        # maximizing\n",
    "        utility = -math.inf\n",
    "    else:\n",
    "        # minimizing\n",
    "        utility = math.inf\n",
    "\n",
    "    for move in state.possible_moves:\n",
    "        tmp_state = make_move(state, move)\n",
    "        tmp_utility = minimax(tmp_state, depth - 1, heuristic, None, None)\n",
    "        if state.turn == WHITE:\n",
    "            # maximizing\n",
    "            utility = max(utility, tmp_utility)\n",
    "        else:\n",
    "            # minimizing\n",
    "            utility = min(utility, tmp_utility)\n",
    "    return utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha-Beta KI\n",
    "Diese \\ac{KI} verwendet den Minimax Algorithmus mit der Optimierung Alpha-Beta Pruning, welche in \\autoref{sec:alphabeta} beschrieben ist, um die Nützlichkeit eines Spielzustands zu bestimmen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Merken der Ergebnisse vorheriger Ausführungen wird das Dictionary `transposition_table` verwendet. Dies ist gerade bei der Verwendung von Iterative Deepening für das Move Ordering vorteilhaft. Der Schlüssel des Dictionaries besteht aus dem Zustand des Spielbretts, dem Spieler, der an der Reihe ist, und der verwendeten Heuristik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposition_table = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `alphabeta` implementiert den Minimax-Algorithmus mit Alpha-Beta-Pruning. Eingabeparameter der Funktion sind der zu bewertende Spielzustand `state`, die maximale Suchtiefe `depth`, die zu verwendende Heuristik `heuristic`, sowie die Werte `alpha` und `beta`, die, wie in \\autoref{sec:alphabeta} beschrieben, jeweils den sicher erreichbaren Nutzen für den maximierenden und minimierenden Spieler angeben und für das Abschneiden von Zweigen verwendet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_ab_count = 0\n",
    "\n",
    "def alphabeta(state, depth, heuristic, alpha, beta):\n",
    "    global debug_ab_count\n",
    "    if state.game_over:\n",
    "        return get_utility(state)\n",
    "    if depth == 0:\n",
    "        key = (state.board.tobytes(), state.turn, heuristic)\n",
    "        if key in transposition_table:\n",
    "            return transposition_table[key][0]\n",
    "        debug_ab_count += 1\n",
    "        h = heuristic(state)\n",
    "        transposition_table[key] = (h, 0)\n",
    "        return h\n",
    "\n",
    "    moves = state.possible_moves\n",
    "    child_states = [make_move(state, move) for move in moves]\n",
    "    ordered_moves = []\n",
    "    for child_state in child_states:\n",
    "        key = (child_state.board.tobytes(), child_state.turn, heuristic)\n",
    "        cached = transposition_table.get(key)\n",
    "        if cached == None:\n",
    "            debug_ab_count += 1\n",
    "            cached = (heuristic(child_state), 0)\n",
    "            transposition_table[key] = cached\n",
    "        ordered_moves.append((cached[0], child_state, cached[1]))\n",
    "    ordered_moves.sort(reverse=(state.turn == WHITE))\n",
    "\n",
    "    if state.turn == WHITE:\n",
    "        # maximizing\n",
    "        utility = -math.inf\n",
    "    else:\n",
    "        # minimizing\n",
    "        utility = math.inf\n",
    "\n",
    "    for (_, tmp_state, cached_depth) in ordered_moves:\n",
    "        tmp_utility = alphabeta(tmp_state, depth-1, heuristic, alpha, beta)\n",
    "        if depth - 1 > cached_depth:\n",
    "            transposition_table[\n",
    "                (tmp_state.board.tobytes(),\n",
    "                 tmp_state.turn, heuristic)\n",
    "            ] = (tmp_utility, depth -1)\n",
    "\n",
    "        if state.turn == WHITE:\n",
    "            # maximizing\n",
    "            utility = max(utility, tmp_utility)\n",
    "            alpha = max(alpha, utility)\n",
    "        else:\n",
    "            # minimizing\n",
    "            utility = min(utility, tmp_utility)\n",
    "            beta = min(beta, utility)\n",
    "        if alpha >= beta:\n",
    "            break  # alpha-beta pruning\n",
    "    return utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProbCut KI\n",
    "An dieser Stelle beginnt die Implementierung der \\ac{KI} mittels des Minimax Algorithmus, Alpha-Beta Pruning und ProbCut. Die im Folgenden definierte Konstante `PERCENTILE` entspricht hierbei dem Term $\\Phi^{-1}(p)$ aus \\autoref{sec:probcut}. Für ein $p$ von $93.3\\%$ hat `PERCENTILE` den Wert $1.5$. `PROBCUT_DEEP_DEPTH` und `PROBCUT_SHALLOW_DEPTH` entsprechen den Variablen $d$ und $d'$ aus \\autoref{sec:probcut} dieser Arbeit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCENTILE = 1.5\n",
    "PROBCUT_DEEP_DEPTH = 4\n",
    "PROBCUT_SHALLOW_DEPTH = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Implementierung der ProbCut Strategie gleicht in großen Teilen der Implementierung der Alpha-Beta Strategie. Jedoch wird  bei jedem Aufruf mit der Tiefe `PROBCUT_DEEP_DEPTH`, zunächst eine Suche mit der Tiefe `PROBCUT_SHALLOW_DEPTH` durchgeführt. Anhand der dabei ermittelten Nützlichkeit wird entsprechend der in \\autoref{sec:probcut} beschriebenen Regeln entschieden, ob eine Tiefe Suche durchgeführt werden muss, oder einer der beiden Grenzwerte `alpha` oder `beta` zurückgegeben werden kann. Zur Abschätzung der für den Probcut Algorithmus benötigten Standardabweichung `sigma` wird eine quadratische Funktion in Abhängigkeit von der Anzahl an Steinen auf dem Spielfeld verwendet. Diese wird im folgenden \\autoref{sec:pcsigma} hergeleitet.  Die Eingabe- und der Rückgabeparameter gleichen der Funktion `alphabeta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_pc_count = 0\n",
    "\n",
    "def probcut(state, depth, heuristic, alpha, beta):\n",
    "    global debug_pc_count\n",
    "    if state.game_over:\n",
    "        return get_utility(state)\n",
    "    if depth == 0:\n",
    "        key = (state.board.tobytes(), state.turn, heuristic)\n",
    "        if key in transposition_table:\n",
    "            return transposition_table[key][0]\n",
    "        debug_pc_count += 1\n",
    "        h = heuristic(state)\n",
    "        transposition_table[key] = (h, 0)\n",
    "        return h\n",
    "\n",
    "    if depth == PROBCUT_DEEP_DEPTH:\n",
    "        num_p = state.num_pieces\n",
    "        if num_p <= 58:\n",
    "            if PROBCUT_DEEP_DEPTH == 4:\n",
    "                sigma = -0.007429 + 0.00276 * num_p - 2.11821e-05 * num_p**2\n",
    "            elif PROBCUT_DEEP_DEPTH == 3:\n",
    "                sigma = -0.008665 + 0.00311 * num_p - 2.96811e-05 * num_p**2\n",
    "            if beta < 1:\n",
    "                bound = PERCENTILE * sigma + beta\n",
    "                if probcut(state, PROBCUT_SHALLOW_DEPTH,\n",
    "                           heuristic, -math.inf, bound) >= bound:\n",
    "                    return beta\n",
    "            if alpha > -1:\n",
    "                bound = -PERCENTILE * sigma + alpha\n",
    "                if probcut(state, PROBCUT_SHALLOW_DEPTH,\n",
    "                           heuristic, bound, math.inf) <= bound:\n",
    "                    return alpha\n",
    "\n",
    "    moves = state.possible_moves\n",
    "    child_states = [make_move(state, move) for move in moves]\n",
    "    ordered_moves = []\n",
    "    for child_state in child_states:\n",
    "        key = (child_state.board.tobytes(), child_state.turn, heuristic)\n",
    "        cached = transposition_table.get(key)\n",
    "        if cached == None:\n",
    "            debug_pc_count += 1\n",
    "            cached = (heuristic(child_state), 0)\n",
    "            transposition_table[key] = cached\n",
    "        ordered_moves.append((cached[0], child_state, cached[1]))\n",
    "    ordered_moves.sort(reverse=(state.turn == WHITE))\n",
    "\n",
    "    if state.turn == WHITE:\n",
    "        # maximizing\n",
    "        utility = -math.inf\n",
    "    else:\n",
    "        # minimizing\n",
    "        utility = math.inf\n",
    "\n",
    "    for (_, tmp_state, cached_depth) in ordered_moves:\n",
    "        tmp_utility = probcut(tmp_state, depth - 1, heuristic, alpha, beta)\n",
    "        if depth - 1 > cached_depth:\n",
    "            transposition_table[\n",
    "                (tmp_state.board.tobytes(),\n",
    "                 tmp_state.turn, heuristic)\n",
    "            ] = (tmp_utility, depth -1)\n",
    "\n",
    "        if state.turn == WHITE:\n",
    "            # maximizing\n",
    "            utility = max(utility, tmp_utility)\n",
    "            alpha = max(alpha, utility)\n",
    "        else:\n",
    "            # minimizing\n",
    "            utility = min(utility, tmp_utility)\n",
    "            beta = min(beta, utility)\n",
    "        if alpha >= beta:\n",
    "            break  # alpha-beta pruning\n",
    "    return utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Durchführen der Züge\n",
    "Die folgenden Funktionen berechnen mithilfe einer angegebenen \\ac{KI} Strategie den nächsten Zug und wenden diesen auf den übergebenen Zustand `state` an. Damit die Strategien nicht völlig deterministisch sind, und somit besser die Stärke der einzelnen Strategien und Heuristiken bestimmt werden kann, wird nicht immer der beste Zug ausgewählt, sondern stattdessen einer der Züge, die innerhalb eines festgelegten Abstands vom besten Zug liegen. Dieser Abstand wird als `SELECTION_TOLERANCE` definiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTION_TOLERANCE = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suche mit fester Tiefe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `ai_make_move` ist die einfachste der Ausführungsfunktionen. Sie bewertet alle durch einen Zug vom Zustand `state` erreichbaren Spielpositionen und wählt aus diesen, wie oben beschrieben, einen der besten Züge aus. Die Bewertung der Spielzustände wird von der als Parameter übergebenen Funktion `ai` vorgenommen, welche eine der im vorherigen Abschnitt definierten Strategie-Funktionen sein kann. Für jeden Zustand wird die Strategie-Funktion genau einmal mit der Tiefe `depth-1` ausgeführt. Das `-1` wird hierbei verwendet, da\n",
    "bereits in der Funktion `ai_make_move` selbst eine Iteration über die Kindzustände durchgeführt wird. Die Strategie-Funktion erhält außerdem den übergebenen Parameter `heuristic`, welcher eine der implementierten Heuristik-Funktionen sein kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_make_move(ai, state, depth, heuristic):\n",
    "    global utilities\n",
    "    if state.game_over:\n",
    "        return\n",
    "    scored_moves = []\n",
    "    if state.turn == WHITE:\n",
    "        # maximizing\n",
    "        for move in state.possible_moves:\n",
    "            new_state = make_move(state, move)\n",
    "            utility = ai(new_state, depth-1, heuristic, -math.inf, math.inf)\n",
    "            scored_moves.append((utility, move))\n",
    "        best_score, _ = max(scored_moves)\n",
    "    else:\n",
    "        # minimizing\n",
    "        for move in state.possible_moves:\n",
    "            new_state = make_move(state, move)\n",
    "            utility = ai(new_state, depth-1, heuristic, -math.inf, math.inf)\n",
    "            scored_moves.append((utility, move))\n",
    "        best_score, _ = min(scored_moves)\n",
    "    utilities[state.turn] = best_score\n",
    "    top_moves = [move for move in scored_moves\n",
    "                 if abs(move[0] - best_score) <= SELECTION_TOLERANCE]\n",
    "    best_move = random.choice(top_moves)[1]\n",
    "    return make_move(state, best_move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Tiefensuche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ausführungsfunktion `ai_make_move_id` unterscheidet sich von `ai_make_move` dadurch, dass eine iterative Tiefensuche durchgeführt wird. Dazu wird die Strategie-Funktion, statt nur einmal mit der vorgegebenen Tiefe aufgerufen zu werden, beginnend von 1 mit immer höherer Suchtiefe aufgerufen. Wird die Tiefe `depth` erreicht, so wird wie auch in `ai_make_move` einer der besten Züge ausgewählt. Durch die Verwendung eines Caches, der `transposition_table`, in den auf Alpha-Beta-Pruning basierenden Strategien, kann durch die Wiederverwendung der Ergebnisse vorheriger Aufrufe ein besseres Move Ordering vorgenommen werden, und somit die Effizienz des Alpha-Beta-Pruning gesteigert werden. Die Idee dabei ist, dass bei ausreichender Suchtiefe die durch besseres Move Ordering erzielten Ersparnisse den durch den mehrfachen Aufruf der Strategie-Funktionen verursachten, Mehraufwand übertreffen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_make_move_id(ai, state, depth, heuristic):\n",
    "    global utilities\n",
    "    if state.game_over:\n",
    "        return\n",
    "    best_move = None\n",
    "    cur_depth = 1\n",
    "    while cur_depth <= depth:\n",
    "        scored_moves = []\n",
    "        if state.turn == WHITE:\n",
    "            # maximizing\n",
    "            for move in state.possible_moves:\n",
    "                new_state = make_move(state, move)\n",
    "                utility = ai(new_state, cur_depth-1,\n",
    "                             heuristic, -math.inf, math.inf)\n",
    "                scored_moves.append((utility, move))\n",
    "            best_score, _ = max(scored_moves)\n",
    "        else:\n",
    "            # minimizing\n",
    "            for move in state.possible_moves:\n",
    "                new_state = make_move(state, move)\n",
    "                utility = ai(new_state, cur_depth-1,\n",
    "                             heuristic, -math.inf, math.inf)\n",
    "                scored_moves.append((utility, move))\n",
    "            best_score, _ = min(scored_moves)\n",
    "        utilities[state.turn] = best_score\n",
    "        top_moves = [move for move in scored_moves\n",
    "                     if abs(move[0] - best_score) <= SELECTION_TOLERANCE]\n",
    "        best_move = random.choice(top_moves)[1]\n",
    "        cur_depth += 1\n",
    "    return make_move(state, best_move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeitbeschränkte Tiefensuche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je nach Spielsituation ist die Mobilität der Spieler unterschiedlich hoch. Dadurch unterscheidet sich auch die Anzahl der zu betrachtenden Spielzustände. Auch die Anzahl der durch Alpha-Beta-Pruning entfernten Zweige kann variieren. Bei konstanter Suchtiefe ist daher mit variablen Ausführungszeiten zu rechnen. Im Spiel gegen einen menschlichen Spieler ist es jedoch wünschenswert, eine maximale Zugdauer nicht zu überschreiten. Die verfügbare Zeit soll dabei dennoch effektiv für eine möglichst gute Entscheidung genutzt werden.\n",
    "\n",
    "Das ist das Ziel der Ausführungsfunktion `ai_make_move_id_timelimited`, diese führt eine iterative Tiefensuche durch, kann jedoch nach jeder Iteration abbrechen und einen der bis dahin besten Züge wählen. Hierbei wird die Entscheidung zum Abbruch getroffen, wenn mit der nächsten Ausführung das durch den Parameter `timelimit` gegebene Zeitlimit voraussichtlich überschritten würde. Dafür wird die Dauer der nächsten Ausführung approximiert, indem bestimmt wird, um welchen Faktor sich die Ausführungszeit bei den letzten beiden Ausführungen geändert hat. Dieser Faktor `factor` wird dann mit der Dauer der letzten Ausführung multipliziert, um die Dauer der nächsten Ausführung zu schätzen. Zu beachten ist, dass die Funktion `ai_make_move_id_timelimited` nicht exakt die gleiche Schnittstelle hat, wie die anderen Ausführungsfunktionen. Der Parameter `depth` wurde hier durch das `timelimit` ersetzt. Dies ist beim Aufruf der Funktion zu beachten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_make_move_id_timelimited(ai, state, timelimit, heuristic):\n",
    "    global utilities\n",
    "    if state.game_over:\n",
    "        return\n",
    "    best_move = None\n",
    "    depth = 1\n",
    "    last_time = 1\n",
    "    second_last_time = 1\n",
    "    factor = 0\n",
    "    start = time.time()\n",
    "    while (\n",
    "        depth <= 64 - state.num_pieces and\n",
    "        timelimit - (time.time() - start) >= factor * last_time\n",
    "    ):\n",
    "        last_time_start = time.time()\n",
    "        scored_moves = []\n",
    "        if state.turn == WHITE:\n",
    "            # maximizing\n",
    "            for move in state.possible_moves:\n",
    "                new_state = make_move(state, move)\n",
    "                utility = ai(new_state, depth-1,\n",
    "                             heuristic, -math.inf, math.inf)\n",
    "                scored_moves.append((utility, move))\n",
    "            best_score, _ = max(scored_moves)\n",
    "        else:\n",
    "            # minimizing\n",
    "            for move in state.possible_moves:\n",
    "                new_state = make_move(state, move)\n",
    "                utility = ai(new_state, depth-1,\n",
    "                             heuristic, -math.inf, math.inf)\n",
    "                scored_moves.append((utility, move))\n",
    "            best_score, _ = min(scored_moves)\n",
    "        utilities[state.turn] = best_score\n",
    "        top_moves = [move for move in scored_moves\n",
    "                     if abs(move[0] - best_score) <= timelimit]\n",
    "        best_move = random.choice(top_moves)[1]\n",
    "        second_last_time = last_time\n",
    "        last_time = time.time() - last_time_start\n",
    "        factor = min(last_time / second_last_time, 3)\n",
    "        depth += 1\n",
    "    print(\"Reached depth\", depth-1, \"in\", time.time() - start, \"seconds\")\n",
    "    return make_move(state, best_move)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
