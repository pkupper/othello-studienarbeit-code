{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KI Implementierung (othello_ai.ipynb)\n",
    "\\label{sec:aiimpl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".container { width:100% }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Kapitel beschreibt die Implemenentierung der Künstlichen Intelligenz. Es werden mehrere, voneinander verschiedene, Strategien implementiert, um einen Vergleich von diesen zu ermöglichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importieren der externen Abhängigkeiten\n",
    "Zur Initialisierung der Parameter `alpha` und `beta` in der mit Alpha-Beta Pruning optimierten Minimax Strategie werden die Konstanten `math.inf` und `-math.inf` benötigt. Sie stehen jeweils für den maximalen und den minimalen Wert, den eine Fließkommazahl annehmen kann. Die Konstanten werden von der Python Standardbibliothek in dem Modul `math` bereitgestellt\n",
    "\n",
    "Das Modul `random` wird im Rahmen dieser Implementierung für mehrere Zwecke genutzt.\n",
    "Zum Einen zur Implementierung der `random_ai`, einer Strategie, welche immer einen zufälligen Zug wählt. Und außerdem, um die auf Minimax basierenden Strategien nichtdeterministisch zu machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Implementierung der Künstlichen Intelligenz baut auf der Implementierung der Spielelogik von Othello auf. Das entsprechende Notebook muss also hier ausgeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run othello_game.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die aktuellen Nutzen-Werte der beiden Spieler werden in der globalen Variable utilities gespeichert, sodass diese in der GUI angezeigt werden können. Diese Werte werden in den entsprechenden Funktionen der Strategien aktualisiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilities = {WHITE: '-', BLACK: '-'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristiken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Abschätzen der Nützlichkeit eines Spielzustands wird eine Heuristik benötigt. Im folgenden sind einige solcher Heuristiken implementiert. Da Weiß der maximierende Spieler, und Schwarz der minimierende Spieler ist, repräsentiert ein höherer Wert der Heuristik einen für Weiß vorteilhaften Zug, während ein niedriger Wert einen Vorteil für Schwarz repräsentiert. Die Werte aller Heuristiken liegen zwischen $-1$ und $1$, wobei diese Randwerte einen garantierten Sieg für den jeweiligen Spieler darstellen. Der Wert $0$ steht für einen für beide Spieler gleich guten Spielzustand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da das Ziel des Spiels Othello ist, zum Ende des Spiels mehr Steine als der Gegner auf dem Spielfeld zu haben, ist es naheliegend, die Differenz der Anzahlen von Steinen beider Spieler zur Abschätzung eines Zuges zu verwenden. Genau das macht die Disk-Count-Heuristik indem sie die Differenz der Anzahlen von Steinen, beider Spieler berechnet und den resultierenden Wert zur Normalisierung durch die maximale Anzahl an Steinen teilt. Bei genauerer Betrachtung ist es jedoch, gerade zu Beginn des Spiels, nicht immer vorteilhaft, den Vorsprung an Steinen zu maximieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_count_heuristic(state):\n",
    "    return (count_disks(state, WHITE) - count_disks(state, BLACK)) / 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine weitere Heuristik ist die Mobilität der Spieler. Diese gibt an, wie viele mögliche Züge ein Spieler im Vergleich zum Gegner hat. Die Idee hinter dieser Heuristik ist, dass ein Spieler dadurch seine Freiheit maximiert, während die Freiheit des Gegners durch eine geringe Anzahl an Zügen eingeschränkt wird. Die Mobilitäts-Heuristik gibt an wie viele mögliche Züge mehr Weiß gegenüber Schwarz im Aktuellen Spielzustand hat. Auch dieser Wert wird durch Division durch die Anzahl an Feldern normalisiert, um die Grenzen von $-1$ und $1$ einzuhalten. Zu beachten ist hier, dass auch die Anzahl möglicher Züge für einen Spieler bestimmt wird, der im Spielzustand gar nicht am Zug ist. Dies ist wirkt zunächst sematisch nicht sinvoll, hat sich jedoch, wie in \\autoref{sec:currentmobility} gezeigt, im Vergleich gegenüber möglichen Alternativen, wie der Verwendung einer durchschnittlichen Mobilität, als effektiv bewiesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobility_heuristic(state):\n",
    "    if state.turn == WHITE:\n",
    "        return (len(state.possible_moves) -\n",
    "                len(get_possible_moves(state, BLACK))) / 64\n",
    "    else:\n",
    "        return (len(get_possible_moves(state, WHITE)) -\n",
    "                len(state.possible_moves)) / 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicht nur die aktuelle, sondern auch die potenzielle Mobilität kann vor allem in frühen Phasen des Spiels wichtig für die Bewertung einer Position sein. Die Funktion `pot_mob_heuristic` berechnet für einen Zustand `state` die Differenz der potenziellen Mobilität beider Spieler. Die potenzielle Mobilität eines Spielers ist gegeben durch die Summe aller freien Felder um gegnerische Spielsteine, da Michael Buro dieses Merkmal als in seiner Dissertation als beste Metrik für die Potenzielle Mobilität ausgemacht hat \\cite[S. 9]{evaluationfunctions}. Das Ergebnis wird durch $3.5$ geteilt, da es im Durchschnitt $3.5$ mal so viele potenzielle Züge wie tatsächliche Züge gibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pot_mob_heuristic(state):\n",
    "    board = list(state.board)\n",
    "    fields = 0\n",
    "    for (x,y) in state.frontier:\n",
    "        for dx, dy in directions:\n",
    "            xi = x + dx\n",
    "            yi = y + dy\n",
    "            if 0 <= xi < 8 and 0 <= yi < 8 and board[xi][yi] != 0:\n",
    "                fields -= board[xi][yi]\n",
    "    # Im Durchschnitt gibt es 3.5 Mal mehr potenzielle wie tatsächliche Züge\n",
    "    fields /= 3.5 \n",
    "    return fields / 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `combined_mobility_heuristic` kombiniert die aktuelle und potenzielle Mobilität, wobei zu Beginn des Spiels die potenzielle Mobilität stärker gewichtet wird und gegen Ende des Spiels die aktuelle Mobilität. Michael Buro beschreibt in seiner Dissertation, dass die potenzielle Mobilität bis 36 Spielsteine auf dem Feld liegen wichtiger für die Bewertung ist, als die aktuelle Mobilität \\cite[S. 9]{evaluationfunctions}. Eigene Tests, welche in Kapitel \\ref{sec:combinedmobility} beschrieben sind, ergeben, dass eine lineare Kombination der beiden Merkmale zu einem guten Ergebnis führt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_mobility_heuristic(state):\n",
    "    act = mobility_heuristic(state)\n",
    "    pot = pot_mob_heuristic(state)\n",
    "    return (1 - state.num_pieces / 50) * pot + (state.num_pieces / 50) *  act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beim Spielen von Othello fällt auf, dass es bestimmte Felder gibt, deren Belegung von Vorteil ist, sowie einige, deren Belegung eher nachteilhaft ist. Diese Eigenschaft macht sich die Cowthello-Heuristik zu Nutze. Diese weist jedem Feld einen Wert zu der angibt, wie Vorteilhaft der Besitz dieses Feldes ist, bzw. wie Nachteilhaft die Belegung des Feldes durch den Gegner ist. Diese Gewichte werden mit der aktuellen Belegung des Spielfelds multipliziert und die Ergebnisse anschließend aufsummiert. Der resultierende Wert schätzt dann den Nutzen der aktuellen Position ein. Auch bei dieser Heuristik findet eine Normalisierung statt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgrund der Symmetrie des Othello Spielfeldes ist es für die Weight-Heuristik nicht nötig, für jedes Feld einzeln dessen Gewicht anzugeben. Stattdessen werden ausschließlich die Gewichte für ein Viertel des Spielfelds angegeben und dieses anschließend gespiegelt. Die Funktion `gen_cowthello_matrix` generiert dann die Gewichte-Matrix für das gesamte Feld und führt auch die Normalisierung durch. Dabei werden die Gewichte aus dem Online-Othello Programm Cowthello verwendet. \\cite{cowthello} Cowthello ist unter der URL <https://www.aurochs.org/games/cowthello/> verfügbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_cowthello_matrix():\n",
    "    quarter = np.array([\n",
    "        [100, -25, 25, 10],\n",
    "        [-25, -50,  1,  1],\n",
    "        [ 25,   1, 50,  5],\n",
    "        [ 10,   1,  5,  1]\n",
    "    ])\n",
    "    top_half = np.hstack((quarter, np.flip(quarter, axis=1)))\n",
    "    bottom_half = np.flip(top_half, axis=0)\n",
    "    raw_matrix = np.vstack((top_half, bottom_half))\n",
    "    max_possible = np.sum(np.absolute(raw_matrix))\n",
    "    return np.true_divide(raw_matrix, max_possible)\n",
    "\n",
    "cowthello_weights = gen_cowthello_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `cowthello_heuristic` bestimmt aus einem Spielzustand und der aufgestellten Gewichtematrix `cowthello_weights` die gewichtete Summe, welche als Heuristik genutzt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cowthello_heuristic(state):\n",
    "    return np.sum(np.multiply(state.board, cowthello_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Cowthello Heuristik wertet den Besitz einiger Felder, wie zum Beispiel die an die Ecken des Spielfeldes angrenzenden Felder, als negativ, da der Gegner dadurch wertvolle Felder, wie die Ecken des Spielfelds, erlangen kann. Wenn die Ecke jedoch bereits besetzt ist, spielt das keine Rolle mehr. Die `cowthello_safe_heuristic` versucht dies zu berücksichtigen.\n",
    "Zunächst wird dabei in der Funktion `safe_in_corner` ausgehend von einer durch `rdir` und `cdir` angegeben Ecke des Spielfeldes `board` bestimmt, welche Steine des Spielers `player` nicht mehr umgedreht werden können. Alle dadurch als sicher bestimmten Felder, werden in der Boolean-Matrix safe auf den Wert `True` gesetzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_in_corner(board, safe, player, rdir, cdir):\n",
    "    safe_in_row = 9\n",
    "    rows = range(8) if rdir == 1 else reversed(range(8))\n",
    "    for row in rows:\n",
    "        i = np.argmax(board[row,::cdir] != player)\n",
    "        safe_in_row = min(i, safe_in_row - 1)\n",
    "        if safe_in_row == 0:\n",
    "            break\n",
    "        safe[row,::cdir][:safe_in_row] = True\n",
    "    safe_in_col = 9\n",
    "    cols = range(8) if cdir == 1 else reversed(range(8))\n",
    "    for col in cols:\n",
    "        i = np.argmax(board[::rdir,col] != player)\n",
    "        safe_in_col = min(i, safe_in_col - 1)\n",
    "        if safe_in_col == 0:\n",
    "            break\n",
    "        safe[::rdir,col][:safe_in_col] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Funktion `safe_pieces` wird die Funktion `safe_in_corner` für alle Ecken des Spielfelds `board` und dem Spieler `player` aufgerufen. Die sicheren Felder werden in der Boolean Matrix `safe` gesammelt, welche von der Funktion zurückgegeben wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_pieces(state, player):\n",
    "    board = state.board\n",
    "    safe = np.zeros((8, 8), dtype=np.bool)\n",
    "    safe_in_corner(board, safe, player, 1, 1)\n",
    "    safe_in_corner(board, safe, player, 1,-1)\n",
    "    safe_in_corner(board, safe, player,-1, 1)\n",
    "    safe_in_corner(board, safe, player,-1,-1)\n",
    "    return safe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die `cowthello_safe_heuristic` bestimmt zunächst für beide Spieler die bereits gesicherten Felder. Die `cowthello_weights` Matrix wird dann so angepasst, dass alle gesicherten Felder positiv gewichtet werden. Dazu wird der absolutwert der vorherigen Gewichtung verwendet. Die Berechnung der Heuristik, funktioniert dann, wie bei der unveränderten `cowthello_heuristic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cowthello_safe_heuristic(state):\n",
    "    black_safe = safe_pieces(state, BLACK)\n",
    "    white_safe = safe_pieces(state, WHITE)\n",
    "    weights = np.copy(cowthello_weights)\n",
    "    weights[black_safe] = abs(weights[black_safe])\n",
    "    weights[white_safe] = abs(weights[white_safe])\n",
    "    return np.sum(np.multiply(state.board, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die oben implementierten Heuristiken bewerten jeweils nur ein Merkmal der aktuellen Spielsitation. Durch eine Kombination mehrerer dieser Heuristiken können mehrere Merkmale gleichzeitig betrachtet werden. Die Gewichtung von Mobilität und Cowthello wird in Kapitel \\ref{sec:mobcowweight} bestimmt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_heuristic(state):\n",
    "    mobility = combined_mobility_heuristic(state)\n",
    "    cowthello = cowthello_safe_heuristic(state)\n",
    "    return 0.625 * mobility + 0.375 * cowthello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung der Strategien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden werden die verschiedenen Strategien der Künstlichen Intelligenz implementiert. Diese verwenden zum Teil die, im vorherigen Kaptitel implementierten, Heuristiken.\n",
    "Die Strategien bestehen jeweils aus einer bewertenden Funktion, die den Nutzen einer Spielsituation bestimmt, sowie aus einer aufrufenden Funktion, welche mithilfe der bewertenden Funktion den bestmöglichen Zug herleitet. Diese Komponenten können beliebig kombiniert werden. Im Folgenden werden zunächst die bewerteten Funktionen implementiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zufällige KI\n",
    "Die zufällige KI bewertet den Nutzen aller Züge gleich, gibt also immer den Wert $0$ zurück.  Da die Strategie-Funktionen völlig austauschbar sein sollen, müssen alle diese Funktionen dieselben Eingabeparameter haben. Im Fall der Funktion `random_ai` wird jedoch keiner der definierten Parameter benötigt. Der Zweck dieser Künstlichen Intelligenz ist die Messung der Stärke, der anderen KIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_ai(state, depth, heuristic, alpha, beta):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimax KI\n",
    "Die Minimax Strategie verwendet den unveränderten Minimax-Algorithmus, wie er in \\autoref{sec:minimax} beschrieben ist, zur Bestimmung der Nützlichkeit eines Zuges. Eingabeparameter, sind hier der zu bewertende Spielzustand `state`, die gewünschte Suchtiefe `depth` sowie die zu verwendende Heuristik\n",
    "`heuristic`. Die Parameter `alpha` und `beta` dienen, wie oben beschrieben, der Kompatibilität mit den folgenden Strategiefunktionen und werden in der `minimax` Funktion nicht verwendet. Der Rückgabeparameter gibt die ermittelte Nützlichkeit des Spielzustands an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_mm_count = 0\n",
    "\n",
    "def minimax(state, depth, heuristic, alpha, beta):\n",
    "    global debug_mm_count\n",
    "    if state.game_over:\n",
    "        return get_utility(state)\n",
    "    if depth == 0:\n",
    "        debug_mm_count += 1\n",
    "        return heuristic(state)\n",
    "\n",
    "    if state.turn == WHITE:\n",
    "        # maximizing\n",
    "        utility = -math.inf\n",
    "    else:\n",
    "        # minimizing\n",
    "        utility = math.inf\n",
    "\n",
    "    for move in state.possible_moves:\n",
    "        tmp_state = make_move(state, move)\n",
    "        tmp_utility = minimax(tmp_state, depth - 1, heuristic, None, None)\n",
    "        if state.turn == WHITE:\n",
    "            # maximizing\n",
    "            utility = max(utility, tmp_utility)\n",
    "        else:\n",
    "            # minimizing\n",
    "            utility = min(utility, tmp_utility)\n",
    "    return utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha-Beta KI\n",
    "Diese KI verwended den Minimax Algorithmus mit der Optimierung Alpha-Beta Pruning, welche in \\autoref{sec:alphabeta} beschrieben ist, um die Nützlichkeit eines Spielzustands zu bestimmen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Merken der Ergebnisse vorheriger Ausführungen wird das Dictionary `transposition_table` verwendet. Dies ist gerade bei der Verwendung von Iterative Deepening für das Move-Ordering vorteilhaft. Der Schlüssel des Dictionaries besteht aus dem Zustand des Spielbretts, dem Spieler, der an der Reihe ist und der verwendeten Heuristik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposition_table = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `alphabeta` implementiert den Minimax-Algorithmus mit Alpha-Beta-Pruning. Eingabeparameter der Funktion sind der zu bewertende Spielzustand `state`, die maximale Suchtiefe `depth`, die zu verwendende Heuristik `heuristic`, sowie die Werte `alpha` und `beta`, die, wie in \\autoref{sec:alphabeta} beschrieben, jeweils die sicher erreichbaren Nützlichkeiten für den maximierenden und minimierenden Spieler angeben und für das Abschneiden von Zweigen verwendet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_ab_count = 0\n",
    "\n",
    "def alphabeta(state, depth, heuristic, alpha, beta):\n",
    "    global debug_ab_count\n",
    "    if state.game_over:\n",
    "        return get_utility(state)\n",
    "    if depth == 0:\n",
    "        debug_ab_count += 1\n",
    "        return heuristic(state)\n",
    "\n",
    "    moves = state.possible_moves\n",
    "    child_states = [make_move(state, move) for move in moves]\n",
    "    ordered_moves = []\n",
    "    for child_state in child_states:\n",
    "        cached = transposition_table.get(\n",
    "            (child_state.board.tobytes(), child_state.turn, heuristic),\n",
    "            (heuristic(child_state), 0)\n",
    "        )\n",
    "        ordered_moves.append((cached[0], child_state, cached[1]))\n",
    "    ordered_moves.sort(reverse=(state.turn == WHITE))\n",
    "\n",
    "    if state.turn == WHITE:\n",
    "        # maximizing\n",
    "        utility = -math.inf\n",
    "    else:\n",
    "        # minimizing\n",
    "        utility = math.inf\n",
    "\n",
    "    for (_, tmp_state, cached_depth) in ordered_moves:\n",
    "        tmp_utility = alphabeta(tmp_state, depth - 1, heuristic, alpha, beta)\n",
    "        if depth - 1 > cached_depth:\n",
    "            transposition_table[\n",
    "                (tmp_state.board.tobytes(),\n",
    "                 tmp_state.turn, heuristic)\n",
    "            ] = (tmp_utility, depth -1)\n",
    "\n",
    "        if state.turn == WHITE:\n",
    "            # maximizing\n",
    "            utility = max(utility, tmp_utility)\n",
    "            alpha = max(alpha, utility)\n",
    "        else:\n",
    "            # minimizing\n",
    "            utility = min(utility, tmp_utility)\n",
    "            beta = min(beta, utility)\n",
    "        if alpha >= beta:\n",
    "            break  # alpha-beta pruning\n",
    "    return utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProbCut KI\n",
    "An dieser Stelle beginnt die Implementierung der Künstlichen Intelligenz mittels des Minimax Algorithmus, Alpha-Beta Pruning und ProbCut. Die im Folgenden definierte Konstante `PERCENTILE` entspricht hierbei der Variable $p$ aus \\autoref{sec:probcut}. `PROBCUT_DEEP_DEPTH` entspricht der Variable $d$ und `PROBCUT_SHALLOW_DEPTH` entspricht der Variable $d'$ aus demselben Abschnitt dieser Arbeit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCENTILE = 1.5  # 93.3%\n",
    "PROBCUT_DEEP_DEPTH = 4\n",
    "PROBCUT_SHALLOW_DEPTH = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Implementierung der ProbCut Strategie gleicht in großen Teilen der Implementierung der Alpha-Beta Strategie. Jedoch wird  bei jedem Aufruf mit der Tiefe `PROBCUT_DEEP_DEPTH`, zunächst eine Suche mit der Tiefe `PROBCUT_SHALLOW_DEPTH` durchgeführt. Anhand der dabei ermittelten Nützlichkeit, wird entsprechend der in \\autoref{sec:probcut} beschriebenen Regeln entschieden, ob eine Tiefe Suche durchgeführt werden muss, oder eine der beiden Grenzwerte `alpha` oder `beta` zurückgegeben werden können. Zur Abschätzung der für den Probcut Algorithmus benötigten Standardabweichung `sigma`, wird eine quadratische Funktion in Abhängigkeit von der Anzahl an steinen auf dem Spielfeld verwendet. Diese wird im folgenden \\autoref{sec:pcsigma} hergeleitet.  Die Eingabe- und der Rückgabeparameter gleichen der Funktion `alphabeta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_pc_count = 0\n",
    "\n",
    "def probcut(state, depth, heuristic, alpha, beta):\n",
    "    global debug_pc_count\n",
    "    if state.game_over:\n",
    "        return get_utility(state)\n",
    "    if depth == 0:\n",
    "        debug_pc_count += 1\n",
    "        return heuristic(state)\n",
    "\n",
    "    if depth == PROBCUT_DEEP_DEPTH:\n",
    "        num_p = state.num_pieces\n",
    "        if num_p <= 58:\n",
    "            sigma = 0.00261067 + 0.00119532 * num_p + 2.65512979e-05 * num_p**2\n",
    "            bound = PERCENTILE * sigma + beta\n",
    "            if probcut(state, PROBCUT_SHALLOW_DEPTH,\n",
    "                       heuristic, bound-1, bound) >= bound:\n",
    "                return beta\n",
    "            \n",
    "            bound = -PERCENTILE * sigma + alpha\n",
    "            if probcut(state, PROBCUT_SHALLOW_DEPTH,\n",
    "                       heuristic, bound, bound+1) <= bound:\n",
    "                return alpha\n",
    "\n",
    "    moves = state.possible_moves\n",
    "    child_states = [make_move(state, move) for move in moves]\n",
    "    ordered_moves = []\n",
    "    for child_state in child_states:\n",
    "        cached = transposition_table.get(\n",
    "            (child_state.board.tobytes(), child_state.turn, heuristic),\n",
    "            (heuristic(child_state), 0)\n",
    "        )\n",
    "        ordered_moves.append((cached[0], child_state, cached[1]))\n",
    "    ordered_moves.sort(reverse=(state.turn == WHITE))\n",
    "\n",
    "    if state.turn == WHITE:\n",
    "        # maximizing\n",
    "        utility = -math.inf\n",
    "    else:\n",
    "        # minimizing\n",
    "        utility = math.inf\n",
    "\n",
    "    for (_, tmp_state, cached_depth) in ordered_moves:\n",
    "        tmp_utility = probcut(tmp_state, depth - 1, heuristic, alpha, beta)\n",
    "        if depth - 1 > cached_depth:\n",
    "            transposition_table[\n",
    "                (tmp_state.board.tobytes(),\n",
    "                 tmp_state.turn, heuristic)\n",
    "            ] = (tmp_utility, depth -1)\n",
    "\n",
    "        if state.turn == WHITE:\n",
    "            # maximizing\n",
    "            utility = max(utility, tmp_utility)\n",
    "            alpha = max(alpha, utility)\n",
    "        else:\n",
    "            # minimizing\n",
    "            utility = min(utility, tmp_utility)\n",
    "            beta = min(beta, utility)\n",
    "        if alpha >= beta:\n",
    "            break  # alpha-beta pruning\n",
    "    return utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Durchführen der Züge\n",
    "Die folgenden Funktionen berechnen mithilfe einer angegebenen KI Strategie den nächsten Zug und wenden diesen auf den übergebenen Zustand `state` an. Damit die Strategien nicht völlig deterministisch sind, und somit besser die Stärke der einzelnen Strategien und Heuristiken bestimmt werden kann, wird nicht immer der beste Zug ausgewählt, sondern stattdessen einer der Züge, die innerhalb eines festgelegten Abstands vom besten Zug liegen. Dieser Abstand wird im Folgenden als `SELECTION_TOLERANCE` definiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTION_TOLERANCE = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suche mit fester Tiefe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `ai_make_move` ist die einfachste der Ausführungsfunktionen. Sie bewertet alle, durch einen Zug vom Zustand `state` erreichbaren, Spielpositionen und wählt aus diesen, wie oben beschrieben, einen der besten Züge aus. Die Bewertung der Spielzustände wird von der als Parameter übergebenen Funktion `ai` vorgenommen, welche eine der im vorherigen Abschnitt definierten Strategie-Funktionen sein kann. Für jeden Zustand wird die Strategie-Funktion genau einmal mit der Tiefe `depth-1` ausgeführt. Das `-1` wird hierbei verwendet, da\n",
    "bereits in der Funktion `ai_make_move` selbst eine Iteration über die Kindzustände durchgeführt wird. Die Strategiefunktion erhält ausßerdem den übergebenen Parameter `heuristic`, welche eine der implementierten Heuristikfunktionen sein kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_make_move(ai, state, depth, heuristic):\n",
    "    global utilities\n",
    "    if state.game_over:\n",
    "        return\n",
    "    scored_moves = []\n",
    "    if state.turn == WHITE:\n",
    "        # maximizing\n",
    "        for move in state.possible_moves:\n",
    "            new_state = make_move(state, move)\n",
    "            utility = ai(new_state, depth-1, heuristic, -math.inf, math.inf)\n",
    "            scored_moves.append((utility, move))\n",
    "        best_score, _ = max(scored_moves)\n",
    "    else:\n",
    "        # minimizing\n",
    "        for move in state.possible_moves:\n",
    "            new_state = make_move(state, move)\n",
    "            utility = ai(new_state, depth-1, heuristic, -math.inf, math.inf)\n",
    "            scored_moves.append((utility, move))\n",
    "        best_score, _ = min(scored_moves)\n",
    "    utilities[state.turn] = best_score\n",
    "    top_moves = [move for move in scored_moves\n",
    "                 if abs(move[0] - best_score) <= SELECTION_TOLERANCE]\n",
    "    best_move = random.choice(top_moves)[1]\n",
    "    return make_move(state, best_move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Tiefensuche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ausführungsfunktion `ai_make_move_id` unterscheidet sich von `ai_make_move` dadurch, dass iterative Tiefensuche durchgeführt wird. Dazu wird die Strategiefunktion, statt nur einmal mit der vorgegebenen Tiefe aufgerufen zu werden, beginnend von 1 mit immer höherer Suchtiefe aufgerufen. Wird die Tiefe `depth` erreicht, so wird einer der besten Züge, wie auch in `ai_make_move` ausgewählt. Durch die Verwendung eines Cache, der `transposition_table`, in den auf Alpha-Beta Prunning basierenden Strategien, kann durch die Wiederverwendung der Ergebnisse vorheriger Aufrufe ein besseres Move Ordering vorgenommen werden, und somit die Effizienz des Alpha-Beta Pruning gesteigert werden. Bei ausreichender Suchtiefe übertreffen die dadurch erzielten Ersparnisse, den zusätzlichen Aufwand, die Strategiefunktionen mehrfach aufzurufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_make_move_id(ai, state, depth, heuristic):\n",
    "    global utilities\n",
    "    if state.game_over:\n",
    "        return\n",
    "    best_move = None\n",
    "    cur_depth = 1\n",
    "    while cur_depth <= depth:\n",
    "        scored_moves = []\n",
    "        if state.turn == WHITE:\n",
    "            # maximizing\n",
    "            for move in state.possible_moves:\n",
    "                new_state = make_move(state, move)\n",
    "                utility = ai(new_state, cur_depth-1, heuristic, -math.inf, math.inf)\n",
    "                scored_moves.append((utility, move))\n",
    "            best_score, _ = max(scored_moves)\n",
    "        else:\n",
    "            # minimizing\n",
    "            for move in state.possible_moves:\n",
    "                new_state = make_move(state, move)\n",
    "                utility = ai(new_state, cur_depth-1, heuristic, -math.inf, math.inf)\n",
    "                scored_moves.append((utility, move))\n",
    "            best_score, _ = min(scored_moves)\n",
    "        utilities[state.turn] = best_score\n",
    "        top_moves = [move for move in scored_moves\n",
    "                     if abs(move[0] - best_score) <= SELECTION_TOLERANCE]\n",
    "        best_move = random.choice(top_moves)[1]\n",
    "        cur_depth += 1\n",
    "    return make_move(state, best_move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeitbeschränkte Tiefensuche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je nach Spielsituation ist die Mobilität der Spieler unterschiedlich hoch. Dadurch unterscheidet sich auch die Anzahl der zu betrachteten Spielzustände. Auch die Anzahl der durch Alpha-Beta Pruning entfernten Zweige kann variieren. Bei konstanter Suchtiefe ist daher mit variablen Ausführungszeiten zu rechnen. Im Spiel gegen einen Menschlichen Spieler ist es jedoch wünschenswert, eine maximale Zugdauer nicht zu überschreiten. Die verfügbare Zeit soll dabei dennoch effektiv für eine möglichst gute Entscheidung genutzt werden.\n",
    "\n",
    "Das ist das Ziel der Ausführungsfunktion `ai_make_move_id_timelimited`, diese führt Iterative Deepening durch, kann jedoch nach jeder Suchtiefe abbrechen und einen der bis dahin besten Züge wählen. Hierbei wird die Entscheidung zum Abbruch getroffen, wenn mit der nächsten Ausführung das durch den Parameter `timelimit` gegebene Zeitlimit voraussichtlich überschritten würde. Dafür wird die Dauer der nächsten Ausführung approximiert, indem bestimmt wird, um welchen Faktor sich die Ausführungszeiten bei den letzten beiden Ausführungen geändert haben. Dieser Faktor `factor` wird dann mit der Dauer der letzten Ausführung multipliziert um die Dauer der nächsten Ausführung zu schätzen. Zu beachten ist, dass diese Funktion nicht exakt die gleiche Schnittstelle hat, wie die anderen Ausführungsfunktionen. Der paramenter `depth` wurde hier durch das `timelimit` ersetzt. Dies ist beim Aufruf der Funktion zu beachten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_make_move_id_timelimited(ai, state, timelimit, heuristic):\n",
    "    global utilities\n",
    "    if state.game_over:\n",
    "        return\n",
    "    best_move = None\n",
    "    depth = 1\n",
    "    last_time = 1\n",
    "    second_last_time = 1\n",
    "    factor = 0\n",
    "    start = time.time()\n",
    "    while (\n",
    "        depth <= 64 - state.num_pieces and\n",
    "        timelimit - (time.time() - start) >= factor * last_time\n",
    "    ):\n",
    "        last_time_start = time.time()\n",
    "        scored_moves = []\n",
    "        if state.turn == WHITE:\n",
    "            # maximizing\n",
    "            for move in state.possible_moves:\n",
    "                new_state = make_move(state, move)\n",
    "                utility = ai(new_state, depth-1, heuristic, -math.inf, math.inf)\n",
    "                scored_moves.append((utility, move))\n",
    "            best_score, _ = max(scored_moves)\n",
    "        else:\n",
    "            # minimizing\n",
    "            for move in state.possible_moves:\n",
    "                new_state = make_move(state, move)\n",
    "                utility = ai(new_state, depth-1, heuristic, -math.inf, math.inf)\n",
    "                scored_moves.append((utility, move))\n",
    "            best_score, _ = min(scored_moves)\n",
    "        utilities[state.turn] = best_score\n",
    "        top_moves = [move for move in scored_moves\n",
    "                     if abs(move[0] - best_score) <= timelimit]\n",
    "        best_move = random.choice(top_moves)[1]\n",
    "        second_last_time = last_time\n",
    "        last_time = time.time() - last_time_start\n",
    "        factor = min(last_time / second_last_time, 3)\n",
    "        depth += 1\n",
    "    print(\"Reached depth\", depth-1, \"in\", time.time() - start, \"seconds\")\n",
    "    return make_move(state, best_move)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
