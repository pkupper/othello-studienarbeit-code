{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".container { width:100% }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementierung einer Künstlichen Intelligenz für das Spiel Othello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiale Konfiguration\n",
    "\n",
    "Importieren von Abhängigkeiten und Konfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run othello_game.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die aktuellen Nutzen-Werte der beiden Spieler werden in einer globalen Variable gespeichert, sodass diese in der GUI angezeigt werden können. Die Werte werden in den entsprechenden Funktionen der Strategien aktualisiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilities = {WHITE: '-', BLACK: '-'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristiken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Abschätzen der Nützlichkeit eines Spielzustands wird eine Heuristik benötigt. Im folgenden sind einige solcher Heuristiken implementiert. Da Weiß der maximierende Spieler, und Schwarz der minimierende Spieler ist repräsentiert ein höherer Wert der Heuristik einen für Weiß vorteilhaften Zug, während ein niedriger Wert einen Vorteil für Schwarz repräsentiert. Die Werte der Heuristik liegen zwischen -1 und 1, wobei die Randwerte einen garantierten Sieg für den jeweiligen Spieler darstellen. Der Wert 0 steht für einen für beide Spieler gleich guten Spielzustand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da das Ziel des Spiels Othello ist, zum Ende des Spiels mehr Steine als der Gegner auf dem Spielfeld zu haben, ist es naheliegend, die Differez der Anzahlen von Steinen beider Spieler zur Abschätzung eines Zuges zu verwenden. Genau das macht die Disk-Count-Heuristik indem sie die Differenz der Steine, beider Spieler berechnet und den resultierenden Wert zur Normalisierung durch die maximale Anzahl an Steinen teilt. Bei genauerer Betrachtung ist es jedoch, gerade zu Beginn des Spiels nicht immer vorteilhaft, den Vorsprung an Steinen zu maximieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_count_heuristic(state):\n",
    "    return (count_disks(state, WHITE) - count_disks(state, BLACK)) / 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine weitere Heuristik ist die Mobilität der Spieler. Diese gibt an wie viele mögliche Züge ein Spieler gegenüber dessen Gegner machen kann. Die Idee bei dieser Heuristik ist, dass ein Spieler dadurch seine Freiheit maximiert, während die Freiheit des Gegners durch eine geringe Anzahl an Zügen eingeschränkt wird. Die Mobilitäts-Heuristik gibt an wie viele möglicher Züge mehr Weiß gegenüber Schwarz im Aktuellen Spielzustand hat. Auch dieser Wert wird durch Division durch die Anzahl an Feldern normalisiert, um die Grenzen von -1 und 1 einzuhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobility_heuristic(state):\n",
    "    if state.turn == WHITE:\n",
    "        return (len(state.possible_moves) -\n",
    "                len(get_possible_moves(state, BLACK))) / 64\n",
    "    else:\n",
    "        return (len(get_possible_moves(state, WHITE)) -\n",
    "                len(state.possible_moves)) / 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicht nur die aktuelle, sondern auch die potentielle Mobilität kann vor allem in frühen Phasen des Spiels wichtig für die Bewertung einer Position sein. Die Funktion `pot_mob_heuristic` berechnet für einen Zustand `state` die Differenz der potentiellen Mobilität beider Spieler. Die potentielle Mobilität eines Spielers ist die Summe aller freien Felder um gegnerische Spielsteine, da Michael Buro dieses Merkmal als in seiner Dissertation als bestes ausgemacht hat. Das Ergebnis wird durch 3.5 geteilt, da es im Durchschnitt 3.5 Mal so viele potentielle Züge wie tatsächliche Züge gibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pot_mob_heuristic(state):\n",
    "    board = list(state.board)\n",
    "    fields = 0\n",
    "    for (x,y) in state.frontier:\n",
    "        for dx, dy in directions:\n",
    "            xi = x + dx\n",
    "            yi = y + dy\n",
    "            if 0 <= xi < 8 and 0 <= yi < 8 and board[xi][yi] != 0:\n",
    "                fields -= board[xi][yi]\n",
    "    # Im Durchschnitt gibt es 3.5 Mal mehr potentielle wie tatsächliche Züge\n",
    "    fields /= 3.5 \n",
    "    return fields / 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `combined_mobility_heuristic` kombiniert die aktuelle und potentielle Mobilität, wobei zu Beginn des Spiels die potentielle Mobilität stärker gewichtet wird und gegen Ende die aktuelle Mobilität. Michael Buro beschreibt in seiner Dissertation, dass die potentielle Mobilität bis 36 Spielsteine auf dem Feld liegen wichtiger für die Bewertung ist, als die aktuelle Mobilität. Dieser Sachverhalt soll durch eine lineare Kombination modelliert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_mobility_heuristic(state):\n",
    "    act = mobility_heuristic(state)\n",
    "    pot = pot_mob_heuristic(state)\n",
    "    return (1 - state.num_pieces / 50) * pot + (state.num_pieces / 50) *  act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beim Spielen von Othello fällt auf, dass es bestimmte Felder gibt, deren Belegung von Vorteil ist, sowie einige, deren Belegung eher nachteilhaft ist. Diese Eigenschaft macht sich die Cowthello-Heuristik zu Nutze. Diese weist jedem Feld einen Wert zu der angibt, wie Vorteilhaft der Besitz dieses Feldes ist, bzw. wie Nachteilhaft die Belegung des Feldes durch den Gegner ist. Diese Gewichte werden dann mit der aktuellen Belegung des Spielfelds multipliziert und die Ergebnisse anschließend aufsummiert. Der resultierende Wert schätzt dann den Nutzen der aktuellen Position ein. Auch bei dieser Heuristik findet eine Normalisierung statt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgrund der Symmetrie des Othello Spielfeldes ist es für die Weight-Heuristik nicht nötig, für jedes Feld einzeln dessen Gewicht anzugeben. Stattdessen werden ausschließlich die Gewichte für ein Viertel des Spielfelds angegeben und dieses anschließend gespiegelt. Die Funktion `gen_cowthello_matrix` generiert dann die Gewichte-Matrix für das gesamte Feld und führt auch die Normalisierung durch. Dabei werden die Gewichte aus dem Online-Othello Programm Cowthello verwendet. Cowthello ist unter der URL <https://www.aurochs.org/games/cowthello/> verfügbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_cowthello_matrix():\n",
    "    quarter = np.array([\n",
    "        [100, -25, 25, 10],\n",
    "        [-25, -50,  1,  1],\n",
    "        [ 25,   1, 50,  5],\n",
    "        [ 10,   1,  5,  1]\n",
    "    ])\n",
    "    top_half = np.hstack((quarter, np.flip(quarter, axis=1)))\n",
    "    bottom_half = np.flip(top_half, axis=0)\n",
    "    raw_matrix = np.vstack((top_half, bottom_half))\n",
    "    max_possible = np.sum(np.absolute(raw_matrix))\n",
    "    return np.true_divide(raw_matrix, max_possible)\n",
    "\n",
    "cowthello_weights = gen_cowthello_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cowthello_heuristic(state):\n",
    "    return np.sum(np.multiply(state.board, cowthello_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_in_corner(board, safe, player, rdir, cdir):\n",
    "    safe_in_row = 9\n",
    "    rows = range(8) if rdir == 1 else reversed(range(8))\n",
    "    for row in rows:\n",
    "        i = np.argmax(board[row,::cdir] != player)\n",
    "        safe_in_row = min(i, safe_in_row - 1)\n",
    "        if safe_in_row == 0:\n",
    "            break\n",
    "        safe[row,::cdir][:safe_in_row] = True\n",
    "    safe_in_col = 9\n",
    "    cols = range(8) if cdir == 1 else reversed(range(8))\n",
    "    for col in cols:\n",
    "        i = np.argmax(board[::rdir,col] != player)\n",
    "        safe_in_col = min(i, safe_in_col - 1)\n",
    "        if safe_in_col == 0:\n",
    "            break\n",
    "        safe[::rdir,col][:safe_in_col] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_pieces(state, player):\n",
    "    board = state.board\n",
    "    safe = numpy.zeros((8, 8), dtype=np.bool)\n",
    "    safe_in_corner(board, safe, player, 1, 1)\n",
    "    safe_in_corner(board, safe, player, 1,-1)\n",
    "    safe_in_corner(board, safe, player,-1, 1)\n",
    "    safe_in_corner(board, safe, player,-1,-1)\n",
    "    return safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cowthello_safe_heuristic(state):\n",
    "    black_safe = safe_pieces(state, BLACK)\n",
    "    white_safe = safe_pieces(state, WHITE)\n",
    "    weights = numpy.copy(cowthello_weights)\n",
    "    weights[black_safe] = abs(weights[black_safe])\n",
    "    weights[white_safe] = abs(weights[white_safe])\n",
    "    return np.sum(np.multiply(state.board, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die oben implementierten Heuristiken bewerten jeweils nur ein Merkmal der aktuellen Spielsitation. Durch eine Kombination mehrerer dieser Heuristiken können mehrere Merkmale gleichzeitig betrachtet werden. Die Kombination der Heuristiken kann in Abhängigkeit der aktuellen Spielphase geschehen. Beispielsweise wird zu Ende des Spiels die Disc-Count-Heuristik wichtiger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_heuristic(state):\n",
    "    if state.num_pieces >= 50:\n",
    "        return disc_count_heuristic(state)\n",
    "    mobility = combined_mobility_heuristic(state)\n",
    "    cowthello = cowthello_safe_heuristic(state)\n",
    "    return 0.5 * mobility + 0.5 * cowthello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung der Strategien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zufällige KI\n",
    "Die zufällige KI bewertet den Nutzen aller Züge gleich, gibt also immer den Wert `0` zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_ai(state, depth, heuristic, alpha, beta):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimax KI\n",
    "Diese KI verwendet den Minimax-Algorithmus zur Bestimmung der Nützlichkeit eines Zuges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_mm_count = 0\n",
    "\n",
    "\n",
    "def minimax(state, depth, heuristic, alpha, beta):\n",
    "    global debug_mm_count\n",
    "    if state.game_over:\n",
    "        return get_utility(state)\n",
    "    if depth == 0:\n",
    "        debug_mm_count += 1\n",
    "        return heuristic(state)\n",
    "\n",
    "    if state.turn == WHITE:\n",
    "        # maximizing\n",
    "        utility = -math.inf\n",
    "    else:\n",
    "        # minimizing\n",
    "        utility = math.inf\n",
    "\n",
    "    for move in state.possible_moves:\n",
    "        tmp_state = make_move(state, move)\n",
    "        tmp_utility = minimax(tmp_state, depth - 1, heuristic, None, None)\n",
    "        if state.turn == WHITE:\n",
    "            # maximizing\n",
    "            utility = max(utility, tmp_utility)\n",
    "        else:\n",
    "            # minimizing\n",
    "            utility = min(utility, tmp_utility)\n",
    "    return utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha-Beta KI\n",
    "Diese KI verwended den Minimax Algorithmus mit der Optimierung Alpha-Beta Pruning, um die Nützlichkeit eines Spielzustands zu bestimmen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Merken vorheriger Ausführungen von wird das Dictionary `transposition_table` verwendet. Dies ist gerade bei der Verwendung von Iterative Deepening für das Move-Ordering vorteilhaft. Der Schlüssel des Dictionaries besteht aus dem Zustand des Spielbretts, dem Spieler, der an der Reihe ist und der verwendeten Heuristik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposition_table = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `alphabeta` implementiert den Minimax Algorithmus mit Alpha-Beta-Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "debug_ab_count = 0\n",
    "\n",
    "\n",
    "def alphabeta(state, depth, heuristic, alpha, beta):\n",
    "    global debug_ab_count\n",
    "    if state.game_over:\n",
    "        return get_utility(state)\n",
    "    if depth == 0:\n",
    "        debug_ab_count += 1\n",
    "        return heuristic(state)\n",
    "\n",
    "    moves = state.possible_moves\n",
    "    child_states = [make_move(state, move) for move in moves]\n",
    "    ordered_moves = []\n",
    "    for child_state in child_states:\n",
    "        cached = transposition_table.get(\n",
    "            (child_state.board.tobytes(), child_state.turn, heuristic),\n",
    "            None\n",
    "        )\n",
    "        if cached != None:\n",
    "            ordered_moves.append((cached, child_state))\n",
    "        else:\n",
    "            ordered_moves.append((heuristic(state), child_state))\n",
    "    ordered_moves.sort(reverse=(state.turn == WHITE))\n",
    "\n",
    "    if state.turn == WHITE:\n",
    "        # maximizing\n",
    "        utility = -math.inf\n",
    "    else:\n",
    "        # minimizing\n",
    "        utility = math.inf\n",
    "\n",
    "    for (_, tmp_state) in ordered_moves:\n",
    "        tmp_utility = alphabeta(tmp_state, depth - 1, heuristic, alpha, beta)\n",
    "        transposition_table[(tmp_state.board.tobytes(),\n",
    "                             tmp_state.turn, heuristic)] = tmp_utility\n",
    "\n",
    "        if state.turn == WHITE:\n",
    "            # maximizing\n",
    "            utility = max(utility, tmp_utility)\n",
    "            alpha = max(alpha, utility)\n",
    "        else:\n",
    "            # minimizing\n",
    "            utility = min(utility, tmp_utility)\n",
    "            beta = min(beta, utility)\n",
    "        if alpha >= beta:\n",
    "            break  # alphabeta pruning\n",
    "    return utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProbCut KI\n",
    "An dieser Stelle beginnt die Implementierung der Künstlichen Intelligenz mittels des Minimax Algorithmus und ProbCut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCENTILE = 1.5  # 93.3%\n",
    "PROBCUT_DEEP_DEPTH = 4\n",
    "PROBCUT_SHALLOW_DEPTH = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_pc_count = 0\n",
    "\n",
    "\n",
    "def probcut(state, depth, heuristic, alpha, beta):\n",
    "    global debug_pc_count\n",
    "    if state.game_over:\n",
    "        return get_utility(state)\n",
    "    if depth == 0:\n",
    "        debug_pc_count += 1\n",
    "        return heuristic(state)\n",
    "\n",
    "    if depth == PROBCUT_DEEP_DEPTH - 1:\n",
    "        if state.num_pieces <= 45:\n",
    "            sigma = state.num_pieces * 0.00160610675723707 + 0.03369559303029926\n",
    "            # v >= beta with prob. of at least p? yes => cutoff\n",
    "            bound = PERCENTILE * sigma + beta\n",
    "            if probcut(state, PROBCUT_SHALLOW_DEPTH,\n",
    "                       heuristic, bound-1, bound) >= bound:\n",
    "                return beta\n",
    "\n",
    "            # v <= alpha with prob. of at least p? yes => cutoff\n",
    "            bound = -PERCENTILE * sigma + alpha\n",
    "            if probcut(state, PROBCUT_SHALLOW_DEPTH,\n",
    "                       heuristic, bound, bound+1) <= bound:\n",
    "                return alpha\n",
    "\n",
    "    moves = state.possible_moves\n",
    "    child_states = [make_move(state, move) for move in moves]\n",
    "    ordered_moves = []\n",
    "    for child_state in child_states:\n",
    "        cached = transposition_table.get(\n",
    "            (child_state.board.tobytes(), child_state.turn, heuristic), None\n",
    "        )\n",
    "        if cached != None:\n",
    "            ordered_moves.append((cached, child_state))\n",
    "        else:\n",
    "            ordered_moves.append((heuristic(state), child_state))\n",
    "    ordered_moves.sort(reverse=(state.turn == WHITE))\n",
    "\n",
    "    if state.turn == WHITE:\n",
    "        # maximizing\n",
    "        utility = -math.inf\n",
    "    else:\n",
    "        # minimizing\n",
    "        utility = math.inf\n",
    "\n",
    "    for (_, tmp_state) in ordered_moves:\n",
    "        tmp_utility = probcut(tmp_state, depth - 1, heuristic, alpha, beta)\n",
    "        transposition_table[(tmp_state.board.tobytes(),\n",
    "                             tmp_state.turn, heuristic)] = tmp_utility\n",
    "\n",
    "        if state.turn == WHITE:\n",
    "            # maximizing\n",
    "            utility = max(utility, tmp_utility)\n",
    "            alpha = max(alpha, utility)\n",
    "        else:\n",
    "            # minimizing\n",
    "            utility = min(utility, tmp_utility)\n",
    "            beta = min(beta, utility)\n",
    "        if alpha >= beta:\n",
    "            break  # alphabeta pruning\n",
    "    return utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Durchführen der Züge\n",
    "Die folgenden Funktionen berechnen mit einer angegebenen KI den nächsten Zug und wenden diesen auf den Zustand an. Der Wert `SELECTION_TOLERANCE` gibt an, wie viel der Nutzen eines Zuges vom besten Nutzen abweichen darf, um dennoch ausgewählt werden zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTION_TOLERANCE = 0.0025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `ai_make_move` führt auf dem Zustand `state` den besten, durch den Algorithmus `ai` bestimmten, Zug aus.\n",
    "Für alle möglichen Züge wird die Nützlichkeit des resultierenden Zustands mithilfe des Algorithmus `ai` bestimmt. Aus allen Zügen wird einer der Züge ausgewählt, der für den Spieler den optimaleln Nutzen hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_make_move(ai, state, depth, heuristic):\n",
    "    global utilities\n",
    "    if state.game_over:\n",
    "        return\n",
    "    scored_moves = []\n",
    "    if state.turn == WHITE:\n",
    "        # maximizing\n",
    "        alpha = -math.inf\n",
    "        for move in state.possible_moves:\n",
    "            new_state = make_move(state, move)\n",
    "            utility = ai(new_state, depth-1, heuristic, alpha, math.inf)\n",
    "            scored_moves.append((utility, move))\n",
    "            alpha = max(alpha, utility)\n",
    "        best_score, _ = max(scored_moves)\n",
    "    else:\n",
    "        # minimizing\n",
    "        beta = math.inf\n",
    "        for move in state.possible_moves:\n",
    "            new_state = make_move(state, move)\n",
    "            utility = ai(new_state, depth-1, heuristic, -math.inf, beta)\n",
    "            scored_moves.append((utility, move))\n",
    "            beta = min(beta, utility)\n",
    "        best_score, _ = min(scored_moves)\n",
    "    utilities[state.turn] = best_score\n",
    "    top_moves = [move for move in scored_moves if abs(move[0] - best_score) <= SELECTION_TOLERANCE]\n",
    "    best_move = random.choice(top_moves)[1]\n",
    "    return make_move(state, best_move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beim Iterative Deepening wird der Alphabeta-Algorithmus nacheinander mit steigender Tiefe ausgeführt. Die Werte aus der vorherigen Ausführung werden dann für das Move-Ordering verwendet. Dadurch das bessere Move-Ordering können beim Alpha-Beta-Pruning mehr Zweige abgeschnitten werden. Dadurch kann trotz der mehrfachen Ausführung eine höhere Performanz bei gleichem Ergebnis erreicht werden. Ein weiterer Vorteil ist, dass das Iterative Deepening jederzeit abgebrochen werden kann. Dadurch kann in einer vorgegebenen Zeit, die in dieser Zeit maximal mögliche Suchtiefe erreicht werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_make_move_id(ai, state, depth, heuristic):\n",
    "    global utilities\n",
    "    if state.game_over:\n",
    "        return\n",
    "    best_move = None\n",
    "    cur_depth = 1\n",
    "    while cur_depth <= depth:\n",
    "        scored_moves = []\n",
    "        if state.turn == WHITE:\n",
    "            # maximizing\n",
    "            alpha = -math.inf\n",
    "            for move in state.possible_moves:\n",
    "                new_state = make_move(state, move)\n",
    "                utility = ai(new_state, cur_depth-1, heuristic, alpha, math.inf)\n",
    "                scored_moves.append((utility, move))\n",
    "                alpha = max(alpha, utility)\n",
    "            best_score, _ = max(scored_moves)\n",
    "        else:\n",
    "            # minimizing\n",
    "            beta = math.inf\n",
    "            for move in state.possible_moves:\n",
    "                new_state = make_move(state, move)\n",
    "                utility = ai(new_state, cur_depth-1, heuristic, -math.inf, beta)\n",
    "                scored_moves.append((utility, move))\n",
    "                beta = min(beta, utility)\n",
    "            best_score, _ = min(scored_moves)\n",
    "        utilities[state.turn] = best_score\n",
    "        top_moves = [move for move in scored_moves\n",
    "                     if abs(move[0] - best_score) <= SELECTION_TOLERANCE]\n",
    "        best_move = random.choice(top_moves)[1]\n",
    "        cur_depth += 1\n",
    "    return make_move(state, best_move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beim Iterative Deepening ist jederzeit der beste Zug der letzten Suchtiefe bekannt. Daher kann der Algorithmus verwendet werden, um in einer vorgegebenen Zeit mit größtmöglicher Suchtiefe zu suchen. Dazu wird die Dauer des nächsten Zugs anhand der Dauer des letzten Zugs geschätzt. Somit kann es vorkommen, dass die Berechnung etwas länger oder kürzer als die gegebene Zeit dauert. Dafür wird das Ergebnis aller Berechnungen verwendet und die letzte Suche muss nicht abgebrochen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SECS_PER_MOVE = 30\n",
    "\n",
    "\n",
    "def ai_make_move_id_timelimited(ai, state, depth, heuristic):\n",
    "    global utilities\n",
    "    if state.game_over:\n",
    "        return\n",
    "    best_move = None\n",
    "    depth = 1\n",
    "    last_time = 1\n",
    "    second_last_time = 1\n",
    "    factor = 0\n",
    "    start = time.time()\n",
    "    while (\n",
    "        depth <= 64 - state.num_pieces and\n",
    "        SECS_PER_MOVE - (time.time() - start) >= factor * last_time\n",
    "    ):\n",
    "        last_time_start = time.time()\n",
    "        scored_moves = []\n",
    "        if state.turn == WHITE:\n",
    "            # maximizing\n",
    "            alpha = -math.inf\n",
    "            for move in state.possible_moves:\n",
    "                new_state = make_move(state, move)\n",
    "                utility = ai(new_state, depth-1, heuristic, alpha, math.inf)\n",
    "                scored_moves.append((utility, move))\n",
    "                alpha = max(alpha, utility)\n",
    "            best_score, _ = max(scored_moves)\n",
    "        else:\n",
    "            # minimizing\n",
    "            beta = math.inf\n",
    "            for move in state.possible_moves:\n",
    "                new_state = make_move(state, move)\n",
    "                utility = ai(new_state, depth-1, heuristic, -math.inf, beta)\n",
    "                scored_moves.append((utility, move))\n",
    "                beta = min(beta, utility)\n",
    "            best_score, _ = min(scored_moves)\n",
    "        utilities[state.turn] = best_score\n",
    "        top_moves = [move for move in scored_moves\n",
    "                     if abs(move[0] - best_score) <= SELECTION_TOLERANCE]\n",
    "        best_move = random.choice(top_moves)[1]\n",
    "        second_last_time = last_time\n",
    "        last_time = time.time() - last_time_start\n",
    "        factor = min(last_time / second_last_time, 3)\n",
    "        depth += 1\n",
    "    print(\"Reached depth\", depth-1, \"in\", time.time() - start, \"seconds\")\n",
    "    return make_move(state, best_move)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
