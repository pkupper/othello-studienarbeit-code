{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementierung einer Künstlichen Intelligenz für das Spiel Othello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiale Konfiguration\n",
    "\n",
    "Importieren von Abhängigkeiten und Konfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Implementierung der Spielelogik findet in einem separaten Jupyter Notebook statt. Diese wird deshalb hier zunächst ausgeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run othello_game.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import time\n",
    "import numpy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristiken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Abschätzen der Nützlichkeit eines Spielzustands wird eine Heuristik benötigt. Im folgenden sind einige solcher Heuristiken implementiert. Da Weiß der maximierende Spieler, und Schwarz der minimierende Spieler ist repräsentiert ein höherer Wert der Heuristik einen für Weiß vorteilhaften Zug, während ein niedriger Wert einen Vorteil für Schwarz repräsentiert. Die Werte der Heuristik liegen zwischen -1 und 1, wobei die Randwerte einen garantierten Sieg für den jeweiligen Spieler darstellen. Der Wert 0 steht für einen für beide Spieler gleich guten Spielzustand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da das Ziel des Spiels Othello ist, zum Ende des Spiels mehr Steine als der Gegner auf dem Spielfeld zu haben, ist es naheliegend, die Differez der Anzahlen von Steinen beider Spieler zur Abschätzung eines Zuges zu verwenden. Genau das macht die Disk-Count-Heuristik indem sie die Differenz der Steine, beider Spieler berechnet und den resultierenden Wert zur Normalisierung durch die maximale Anzahl an Steinen teilt. Bei genauerer Betrachtung ist es jedoch, gerade zu Beginn des Spiels nicht immer vorteilhaft, den Vorsprung an Steinen zu maximieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_count_heuristic(state):\n",
    "    return (count_disks(state, WHITE) - count_disks(state, BLACK)) / 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine weitere Heuristik ist die Mobilität der Spieler. Diese gibt an wie viele mögliche Züge ein Spieler gegenüber dessen Gegner machen kann. Die Idee bei dieser Heuristik ist, dass ein Spieler dadurch seine Freiheit maximiert, während die Freiheit des Gegners durch eine geringe Anzahl an Zügen eingeschränkt wird. Die Mobilitäts-Heuristik gibt an wie viele möglicher Züge mehr Weiß gegenüber Schwarz im Aktuellen Spielzustand hat. Auch dieser Wert wird durch Division durch die Anzahl an Feldern normalisiert, um die Grenzen von -1 und 1 einzuhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobility_heuristic(state):\n",
    "    if state.turn == WHITE:\n",
    "        return (len(state.possible_moves) - len(get_possible_moves(state, BLACK))) / 64\n",
    "    else:\n",
    "        return (len(get_possible_moves(state, WHITE)) - len(state.possible_moves)) / 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beim Spielen von Othello fällt auf, dass es bestimmte Felder gibt, deren Belegung von Vorteil ist, sowie einige, deren Belegung eher nachteilhaft ist. Diese Eigenschaft macht sich die Weight-Heuristik zu Nutze. Diese weist jedem Feld einen Wert zu der angibt, wie Vorteilhaft der Besitz dieses Feldes ist, bzw. wie Nachteilhaft die Belegung des Feldes durch den Gegner ist. Diese Gewichte werden dann mit der aktuellen Belegung des Spielfelds multipliziert und die Ergebnisse anschließend aufsummiert. Der resultierende Wert schätzt dann den Nutzen der aktuellen Position ein. Auch bei dieser Heuristik findet eine Normalisierung statt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgrund der Symmetrie des Othello Spielfeldes ist es für die Weight-Heuristik nicht nötig, für jedes Feld einzeln dessen Gewicht anzugeben, stattdessen wird das Gewicht jeweils für charakteristische felder, wie zum Beispiel die Ecken, angegeben. Die Funktion `gen_weight_matrix` generiert dann die Gewichte-Matrix für das gesamte Feld und führt auch die Normalisierung durch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_weight_matrix(default=0, corner=0, adj_corner=0, sup_corner=0, edge=0, dia_corner=0, support=0, sup_edge=0):\n",
    "    raw_matrix = numpy.array([\n",
    "        [corner,      adj_corner,  sup_corner,  edge,      edge,      sup_corner,  adj_corner,  corner],\n",
    "        [adj_corner,  dia_corner,  default,     default,   default,   default,     dia_corner,  adj_corner],\n",
    "        [sup_corner,  default,     support,     sup_edge,  sup_edge,  support,     default,     sup_corner],\n",
    "        [edge,        default,     sup_edge,    default,   default,   sup_edge,    default,     edge],\n",
    "        [edge,        default,     sup_edge,    default,   default,   sup_edge,    default,     edge],\n",
    "        [sup_corner,  default,     support,     sup_edge,  sup_edge,  support,     default,     sup_corner],\n",
    "        [adj_corner,  dia_corner,  default,     default,   default,   default,     dia_corner,  adj_corner],\n",
    "        [corner,      adj_corner,  sup_corner,  edge,      edge,      sup_corner,  adj_corner,  corner]\n",
    "    ])\n",
    "    max_possible = numpy.sum(numpy.absolute(raw_matrix))\n",
    "    return numpy.true_divide(raw_matrix, max_possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `weight_heuristic` berechnet die Weight-Heuristik allgemein für eine Gewichte-Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_heuristic(state, weights):\n",
    "    return numpy.sum(numpy.multiply(state.board, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Gewichte-Matrix `cowthello_weights` nutzt die Gewichte aus dem Online-Othello Programm Cowthello. Cowthello ist unter der URL <https://www.aurochs.org/games/cowthello/> verfügbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cowthello_weights = gen_weight_matrix(default=1, corner=100, adj_corner=-25, sup_corner=25, edge=10, dia_corner=-50, support=50, sup_edge=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `cowthello_heuristic` ist eine Weight-Heuristik welche die Gewichte-Matrix von Cowthello verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cowthello_heuristic(state):\n",
    "    return weight_heuristic(state, cowthello_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die oben implementierten Heuristiken bewerten jeweils nur ein Merkmal der aktuellen Spielsitation. Durch eine Kombination mehrerer dieser Heuristiken können mehrere Merkmale gleichzeitig betrachtet werden. Die Kombination der Heuristiken kann in Abhängigkeit der aktuellen Spielphase geschehen. Beispielsweise wird zu Ende des Spiels die Disc-Count-Heuristik wichtiger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_heuristic(state):\n",
    "    if(state.num_pieces >= 50):\n",
    "        return disc_count_heuristic(state)\n",
    "    mobility = mobility_heuristic(state)\n",
    "    cowthello = cowthello_heuristic(state)\n",
    "    return (mobility + cowthello) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung der Strategien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zufällige KI\n",
    "Diese KI wählt aus der Menge der Möglichen Züge einen zufälligen aus und spielt diesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_ai_make_move(state, heuristic):\n",
    "    possible_moves = state.possible_moves\n",
    "    random_move = random.choice(possible_moves)\n",
    "    make_move(state, random_move[0], random_move[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimax KI\n",
    "Diese KI verwendet den Minimax Algorithmus zur Bestimmung der Nützlichkeit eines Zuges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIMAX_DEPTH_LIMIT = 3\n",
    "debug_mm_count= 0\n",
    "\n",
    "def minimax(state, depth, heuristic):\n",
    "    global debug_mm_count\n",
    "    if(state.game_over):\n",
    "        return get_winner(state)\n",
    "    if(depth <= 0):\n",
    "        return heuristic(state)\n",
    "    \n",
    "    if state.turn == WHITE:\n",
    "        # maximizing\n",
    "        utility = -math.inf\n",
    "    else:\n",
    "        # minimizing\n",
    "        utility = math.inf\n",
    "        \n",
    "    for move in get_possible_moves(state, state.turn):\n",
    "        debug_mm_count += 1\n",
    "        tmp_state = copy.deepcopy(state)\n",
    "        make_move(tmp_state, move[0], move[1])\n",
    "        tmp_utility = minimax(tmp_state, depth - 1, heuristic)\n",
    "        if state.turn == WHITE:\n",
    "            # maximizing\n",
    "            utility = max(utility, tmp_utility)\n",
    "        else:\n",
    "            # minimizing\n",
    "            utility = min(utility, tmp_utility)          \n",
    "    return utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für alle möglichen Züge wird die Nützlichkeit des resultierenden Zustands mithilfe von `minimax` bestimmt und aus allen Zügen mit einer, für den Spieler optimalen, Nützlichkeit ein zufälliger Zug ausgewählt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimax_ai_make_move(state, heuristic):\n",
    "    if(state.game_over):\n",
    "        return\n",
    "    scored_moves = [(minimax(make_move(copy.deepcopy(state), move[0], move[1]), MINIMAX_DEPTH_LIMIT-1, heuristic), move) for move in state.possible_moves]\n",
    "    if state.turn == WHITE:\n",
    "        # maximizing\n",
    "        best_score, _ = max(scored_moves)\n",
    "    else:\n",
    "        # minimizing\n",
    "        best_score, _ = min(scored_moves)\n",
    "    best_move = random.choice([move for move in scored_moves if move[0] == best_score])[1]\n",
    "    make_move(state, best_move[0], best_move[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha-Beta KI\n",
    "Diese KI verwended den Minimax Algorithmus mit der Optimierung Alpha-Beta Pruning, um die Nützlichkeit eines Spielzustands zu bestimmen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Merken vorheriger Ausführungen von wird das Dictionary `transposition_table` verwendet. Dies ist gerade bei der Verwendung von Iterative Deepening für das Move-Ordering vorteilhaft. Der Schlüssel des Dictionaries besteht aus dem Zustand des Spielbretts, dem Spieler, der an der Reihe ist und der verwendeten Heuristik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposition_table = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `alphabeta` implementiert den Minimax Algorithmus mit Alpha-Beta-Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABETA_DEPTH_LIMIT = 3\n",
    "debug_ab_count= 0\n",
    "\n",
    "def alphabeta(state, depth, alpha, beta, heuristic):\n",
    "    global debug_ab_count\n",
    "    if(state.game_over):\n",
    "        return get_winner(state)\n",
    "    if(depth <= 0):\n",
    "        return heuristic(state)\n",
    "    \n",
    "    moves = get_possible_moves(state, state.turn)\n",
    "    child_states = [make_move(copy.deepcopy(state), move[0], move[1]) for move in moves]\n",
    "    estimated_utilities = [transposition_table[(child_state.board.tobytes(), child_state.turn, heuristic)]\n",
    "                           if (child_state.board.tobytes(), child_state.turn, heuristic) in transposition_table\n",
    "                           else heuristic(state)\n",
    "                           for child_state in child_states]\n",
    "    ordered_moves = [(estimated_utilities[i], child_states[i]) for i in range(len(moves))]\n",
    "    ordered_moves.sort(reverse=(state.turn == WHITE))\n",
    "    \n",
    "    if state.turn == WHITE:\n",
    "        # maximizing\n",
    "        utility = -math.inf\n",
    "    else:\n",
    "        # minimizing\n",
    "        utility = math.inf\n",
    "        \n",
    "    for (_, tmp_state) in ordered_moves:\n",
    "        debug_ab_count += 1\n",
    "        tmp_utility = alphabeta(tmp_state, depth - 1, alpha, beta, heuristic)\n",
    "        transposition_table[(tmp_state.board.tobytes(), tmp_state.turn, heuristic)] = tmp_utility\n",
    "        \n",
    "        if state.turn == WHITE:\n",
    "            # maximizing\n",
    "            utility = max(utility, tmp_utility)\n",
    "            alpha = max(alpha, utility)\n",
    "        else:\n",
    "            # minimizing\n",
    "            utility = min(utility, tmp_utility)\n",
    "            beta = min(beta, utility)\n",
    "        if(alpha >= beta):\n",
    "            break # alphabeta pruning\n",
    "    return utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie bei der Minimax KI wird auch bei der Alpha-Beta KI für alle möglichen Züge wird die Nützlichkeit des resultierenden Zustands bestimmt, hier mithilfe von von `alphabeta`, und aus allen Zügen mit einer, für den Spieler optimalen, Nützlichkeit ein zufälliger Zug ausgewählt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabeta_ai_make_move(state, heuristic):\n",
    "    if(state.game_over):\n",
    "        return\n",
    "    scored_moves = [(alphabeta(make_move(copy.deepcopy(state), move[0], move[1]), ALPHABETA_DEPTH_LIMIT-1, -math.inf, math.inf, heuristic), move) for move in state.possible_moves]\n",
    "    if state.turn == WHITE:\n",
    "        # maximizing\n",
    "        best_score, _ = max(scored_moves)\n",
    "    else:\n",
    "        # minimizing\n",
    "        best_score, _ = min(scored_moves)\n",
    "    best_move = random.choice([move for move in scored_moves if move[0] == best_score])[1]\n",
    "    make_move(state, best_move[0], best_move[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beim Iterative Deepening wird der Alphabeta-Algorithmus nacheinander mit steigender Tiefe ausgeführt. Die Werte aus der vorherigen Ausführung werden dann für das Move-Ordering verwendet. Dadurch das bessere Move-Ordering können beim Alpha-Beta-Pruning mehr Zweige abgeschnitten werden. Dadurch kann trotz der mehrfachen Ausführung eine höhere Performanz bei gleichem Ergebnis erreicht werden. Ein weiterer Vorteil ist, dass das Iterative Deepening jederzeit abgebrochen werden kann. Dadurch kann in einer vorgegebenen Zeit, die in dieser Zeit maximal mögliche Suchtiefe erreicht werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabeta_id_make_move(state, heuristic):\n",
    "    best_move = None\n",
    "    depth = 1\n",
    "    while depth <= ALPHABETA_DEPTH_LIMIT:\n",
    "        scored_moves = [(alphabeta(make_move(copy.deepcopy(state), move[0], move[1]), depth-1, -math.inf, math.inf, heuristic), move) for move in state.possible_moves]\n",
    "        if state.turn == WHITE:\n",
    "            # maximizing\n",
    "            best_score, _ = max(scored_moves)\n",
    "        else:\n",
    "            # minimizing\n",
    "            best_score, _ = min(scored_moves)\n",
    "        best_move = random.choice([move for move in scored_moves if move[0] == best_score])[1]\n",
    "        depth += 1\n",
    "    make_move(state, best_move[0], best_move[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beim Iterative Deepening ist jederzeit der beste Zug der letzten Suchtiefe bekannt. Daher kann der Algorithmus verwendet werden, um in einer vorgegebenen Zeit mit größtmöglicher Suchtiefe zu suchen. Dazu wird die Dauer des nächsten Zugs anhand der Dauer des letzten Zugs geschätzt. Somit kann es vorkommen, dass die Berechnung etwas länger oder kürzer als die gegebene Zeit dauert. Dafür wird das Ergebnis aller Berechnungen verwendet und die letzte Suche muss nicht abgebrochen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SECS_PER_MOVE = 30\n",
    "\n",
    "def alphabeta_id_make_move_timelimited(state, heuristic):\n",
    "    best_move = None\n",
    "    depth = 1\n",
    "    start = time.time()\n",
    "    last_time = 0\n",
    "    while MAX_SECS_PER_MOVE - (time.time() - start) >= 4 * last_time:\n",
    "        last_time_start = time.time()\n",
    "        scored_moves = [(alphabeta(make_move(copy.deepcopy(state), move[0], move[1]), depth-1, -math.inf, math.inf, heuristic), move) for move in state.possible_moves]\n",
    "        if state.turn == WHITE:\n",
    "            # maximizing\n",
    "            best_score, _ = max(scored_moves)\n",
    "        else:\n",
    "            # minimizing\n",
    "            best_score, _ = min(scored_moves)\n",
    "        best_move = random.choice([move for move in scored_moves if move[0] == best_score])[1]\n",
    "        last_time = time.time() - last_time_start\n",
    "        depth += 1\n",
    "    print(\"Reached depth: \", depth-1)\n",
    "    make_move(state, best_move[0], best_move[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProbCut KI\n",
    "An dieser Stelle beginnt die Implementierung der Künstlichen Intelligenz mittels des Minimax Algorithmus und ProbCut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCENTILE = 1.5 # 93.3%\n",
    "PROBCUT_DEPTH_LIMIT = 5\n",
    "PROBCUT_DEEP_DEPTH = 4\n",
    "PROBCUT_SHALLOW_DEPTH = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_pc_count= 0\n",
    "\n",
    "def probcut(state, depth, alpha, beta, heuristic):\n",
    "    global debug_pc_count\n",
    "    if(state.game_over):\n",
    "        return get_winner(state)\n",
    "    if(depth <= 0):\n",
    "        return heuristic(state)\n",
    "    \n",
    "    if depth == PROBCUT_DEEP_DEPTH - 1:\n",
    "        if state.num_pieces <= 45: # if there are not more than 45 pieces on the board sigma can be calculated based on statistic values with the combined_heuristic\n",
    "            sigma = state.num_pieces * 0.001948723205790693 - 0.001604049443613545\n",
    "            # v >= beta with prob. of at least p? yes => cutoff\n",
    "            bound = PERCENTILE * sigma + beta\n",
    "            if probcut(state, PROBCUT_SHALLOW_DEPTH, bound-1, bound, heuristic) >= bound:\n",
    "                return beta\n",
    "\n",
    "            # v <= alpha with prob. of at least p? yes => cutoff\n",
    "            bound = -PERCENTILE * sigma + alpha\n",
    "            if probcut(state, PROBCUT_SHALLOW_DEPTH, bound, bound+1, heuristic) <= bound:\n",
    "                return alpha\n",
    "    \n",
    "    moves = get_possible_moves(state, state.turn)\n",
    "    child_states = [make_move(copy.deepcopy(state), move[0], move[1]) for move in moves]\n",
    "    estimated_utilities = [transposition_table[(child_state.board.tobytes(), child_state.turn, heuristic)]\n",
    "                           if (child_state.board.tobytes(), child_state.turn, heuristic) in transposition_table\n",
    "                           else heuristic(state)\n",
    "                           for child_state in child_states]\n",
    "    ordered_moves = [(estimated_utilities[i], child_states[i]) for i in range(len(moves))]\n",
    "    ordered_moves.sort(reverse=(state.turn == WHITE))\n",
    "    \n",
    "    if state.turn == WHITE:\n",
    "        # maximizing\n",
    "        utility = -math.inf\n",
    "    else:\n",
    "        # minimizing\n",
    "        utility = math.inf\n",
    "        \n",
    "    for (_, tmp_state) in ordered_moves:\n",
    "        debug_pc_count += 1\n",
    "        tmp_utility = probcut(tmp_state, depth - 1, alpha, beta, heuristic)\n",
    "        transposition_table[(tmp_state.board.tobytes(), tmp_state.turn, heuristic)] = tmp_utility\n",
    "        \n",
    "        if state.turn == WHITE:\n",
    "            # maximizing\n",
    "            utility = max(utility, tmp_utility)\n",
    "            alpha = max(alpha, utility)\n",
    "        else:\n",
    "            # minimizing\n",
    "            utility = min(utility, tmp_utility)\n",
    "            beta = min(beta, utility)\n",
    "        if(alpha >= beta):\n",
    "            break # alphabeta pruning\n",
    "    return utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probcut_ai_make_move(state, heuristic):\n",
    "    if(state.game_over):\n",
    "        return\n",
    "    scored_moves = [(probcut(make_move(copy.deepcopy(state), move[0], move[1]), PROBCUT_DEPTH_LIMIT-1, -math.inf, math.inf, heuristic), move) for move in state.possible_moves]\n",
    "    if state.turn == WHITE:\n",
    "        # maximizing\n",
    "        best_score, _ = max(scored_moves)\n",
    "    else:\n",
    "        # minimizing\n",
    "        best_score, _ = min(scored_moves)\n",
    "    best_move = random.choice([move for move in scored_moves if move[0] == best_score])[1]\n",
    "    make_move(state, best_move[0], best_move[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ermitteln der Standardabweichung\n",
    "Im Folgenden werden zunächst einige Datenpunkte gesammelt, indem in verschiedenen Spielzuständen jeweils eine tiefe, und eine flache Suche durchgeführt wird. Die Spielzustände werden durch zufälliges Ziehen erreicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "filename = f'probcut_dataset_{PROBCUT_SHALLOW_DEPTH}_{PROBCUT_DEEP_DEPTH}.csv'\n",
    "\n",
    "file_empty = not os.path.isfile(filename) or os.stat(filename).st_size == 0\n",
    "with open(filename, 'a+', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    if file_empty:\n",
    "        writer.writerow(('moves', 'shallow', 'deep'))\n",
    "    while True:\n",
    "        state = GameState()\n",
    "        try:\n",
    "            while not state.game_over:\n",
    "                random_ai_make_move(state, None)\n",
    "                shallow_value = alphabeta(state, PROBCUT_SHALLOW_DEPTH, -math.inf, math.inf, combined_heuristic)\n",
    "                deep_value = alphabeta(state, PROBCUT_DEEP_DEPTH, -math.inf, math.inf, combined_heuristic)\n",
    "                print(f'shallow: {shallow_value}, deep: {deep_value}')\n",
    "                writer.writerow((state.num_pieces, shallow_value, deep_value)) \n",
    "        except KeyboardInterrupt:\n",
    "            file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daten Laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "filename = 'probcut_dataset_2_4.csv'\n",
    "df = pandas.read_csv(filename)\n",
    "shallow = df['shallow']\n",
    "deep = df['deep']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berechnung der Standardabweichung nach Anzahl der Steine auf dem Spielfeld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as skl\n",
    "import sklearn.linear_model as lm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "shallow_np = np.array(shallow)\n",
    "deep_np = np.array(deep)\n",
    "moves = np.array(df['moves'])\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for i in range(45-5):\n",
    "    shallow_c = shallow[moves == i+5]\n",
    "    deep_c = deep[moves == i+5]\n",
    "    variance = np.var(np.stack([shallow_c, deep_c], axis=1))\n",
    "    explained_variance = skl.explained_variance_score(shallow_c, deep_c)\n",
    "    x.append(i+5)\n",
    "    y.append(math.sqrt(variance))\n",
    "    \n",
    "model = lm.LinearRegression()\n",
    "model.fit(np.array(x).reshape(-1, 1), np.array(y))\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.set(style='whitegrid')\n",
    "plt.scatter(x, y)\n",
    "plt.axvline(x=0.0, c='k')\n",
    "plt.axhline(y=0.0, c='k')\n",
    "plt.plot(x, x * model.coef_ + model.intercept_)\n",
    "plt.xlabel('number of disks on the board')\n",
    "plt.ylabel('standard deviation')\n",
    "plt.title('Standard Deviation')\n",
    "plt.show()\n",
    "\n",
    "print('x *', model.coef_[0], '+', model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daten Visualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as lm\n",
    "import seaborn as sns\n",
    "model = lm.LinearRegression()\n",
    "model.fit(shallow.values.reshape(len(shallow), 1), deep)\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.set(style='whitegrid')\n",
    "plt.scatter(shallow, deep)\n",
    "plt.axvline(x=0.0, c='k')\n",
    "plt.axhline(y=0.0, c='k')\n",
    "plt.plot(shallow, shallow * model.coef_ + model.intercept_)\n",
    "plt.xlabel('shallow search')\n",
    "plt.ylabel('deep search')\n",
    "plt.title('Probcut Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varianz, erklärte Varianz und Standardabweichung berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as skl\n",
    "variance = np.var(np.stack([shallow, deep], axis=1))\n",
    "print(f'variance: {variance}')\n",
    "explained_variance = skl.explained_variance_score(shallow, deep)\n",
    "print(f'standard distribution: {np.sqrt(variance)}')\n",
    "print(f'explained variance: {explained_variance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applikation Starten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden wird für beide Spieler die zu verwendende Künstliche Intelligenz, sowie die jeweils angewandte Heuristik festgelegt. Ein Wert von `None` bei der KI steht hierbei für einen menschlichen Spieler. Die KIs und Heuristiken werden jeweils in einem Dictionary gespeicher, sodass mit Spieler als Index auf die entsprechende KI oder Heuristik zugegriffen werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "BLACK_PLAYER_AI = random_ai_make_move\n",
    "WHITE_PLAYER_AI = probcut_ai_make_move\n",
    "\n",
    "BLACK_PLAYER_HEURISTIC = combined_heuristic\n",
    "WHITE_PLAYER_HEURISTIC = combined_heuristic\n",
    "\n",
    "PLAYER_AI = {BLACK: BLACK_PLAYER_AI, WHITE: WHITE_PLAYER_AI}\n",
    "PLAYER_HEURISTIC = {BLACK: BLACK_PLAYER_HEURISTIC, WHITE: WHITE_PLAYER_HEURISTIC}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgender Code dient zum Starten der interaktiven Applikation. Die Funktion `next_move` wird für jeden Spielzug ausgeführt. Sie wird zu Beginn einmal aufgerufen. Wenn eine KI spielt, wird die Funktion rekursiv für den nächsten Zug aufgerufen. Wenn der Spieler menschlich ist, muss die Ausführung unterbrochen werden, da auf das Aufrufen eines Callbacks durch einen Klick gewartet werden muss. Im Callback wird auch die Funktion `next_move` für den nächsten Zug aufgerufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = GameState()\n",
    "display_board(state)\n",
    "\n",
    "def next_move(state):\n",
    "    # Check if/which AI is playing\n",
    "    strat = PLAYER_AI[state.turn]\n",
    "    if strat is not None:\n",
    "        time.sleep(0.2)\n",
    "        strat(state, PLAYER_HEURISTIC[state.turn])\n",
    "        update_output(state)\n",
    "        if not state.game_over:\n",
    "            next_move(state)\n",
    "\n",
    "try:\n",
    "    next_move(state)\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der folgende Code dient zum Test, sowie zum Debuggen der oben implementierten Funktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_board = GameState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabeta_ai_make_move(test_board, combined_heuristic)\n",
    "test_board.board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit Hilfe des Pakets `cProfile` lassen sich Statistiken darüber sammeln, wie viel Zeit in welchen Funktionen benötigt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "\n",
    "cProfile.run('alphabeta_ai_make_move(test_board, combined_heuristic)')\n",
    "test_board.board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `debug_num_visited_states` berechnet für einen Spielzustand den nächsten Zug mit verschiedenen Algorithmen und misst dabei die benötigte Zeit. Zusätzlich werden mit den globalen Variablen `debug_mm_count` und  `debug_ab_count` die Anzahl der überprüften Zustände gezählt. Diese Zahlen geben einen Überblick darüber, wie viele Zweige durch den Algorithmus ausgeschlossen werden konnten und nicht überprüft werden mussten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_num_visited_states(state, depth):\n",
    "    global transposition_table\n",
    "    global debug_mm_count\n",
    "    global debug_ab_count\n",
    "    global debug_pc_count\n",
    "    global MINIMAX_DEPTH_LIMIT\n",
    "    global ALPHABETA_DEPTH_LIMIT\n",
    "    global PROBCUT_DEPTH_LIMIT\n",
    "\n",
    "    # save old variables\n",
    "    old_mm_depth = MINIMAX_DEPTH_LIMIT\n",
    "    old_ab_depth = ALPHABETA_DEPTH_LIMIT\n",
    "    old_pb_depth = PROBCUT_DEPTH_LIMIT\n",
    "    \n",
    "    # set depth limit for all algorithms\n",
    "    MINIMAX_DEPTH_LIMIT = depth\n",
    "    ALPHABETA_DEPTH_LIMIT = depth\n",
    "    PROBCUT_DEPTH_LIMIT = depth\n",
    "\n",
    "    # calculate next move with each algorithm and measure time\n",
    "    \"\"\"debug_mm_count= 0\n",
    "    start = time.time()\n",
    "    minimax_ai_make_move(copy.deepcopy(state), combined_heuristic)\n",
    "    secs = time.time() - start\n",
    "    print(\"Minimax takes\", secs, \"seconds and checks\", debug_mm_count, \"substates\")\n",
    "    debug_ab_count= 0\n",
    "    transposition_table = {}\n",
    "    start = time.time()\n",
    "    alphabeta_ai_make_move(copy.deepcopy(state), combined_heuristic)\n",
    "    secs = time.time() - start\n",
    "    print(\"AlphaBeta takes\", secs, \"seconds and checks\", debug_ab_count, \"substates\")\"\"\"\n",
    "    debug_ab_count= 0\n",
    "    transposition_table = {}\n",
    "    start = time.time()\n",
    "    alphabeta_id_make_move(copy.deepcopy(state), combined_heuristic)\n",
    "    secs = time.time() - start\n",
    "    print(\"AlphaBeta with iterative deepening takes\", secs, \"seconds and checks\", debug_ab_count, \"substates\")\n",
    "    debug_pc_count= 0\n",
    "    transposition_table = {}\n",
    "    start = time.time()\n",
    "    probcut_ai_make_move(copy.deepcopy(state), combined_heuristic)\n",
    "    secs = time.time() - start\n",
    "    print(\"ProbCut takes\", secs, \"seconds and checks\", debug_pc_count, \"substates\")\n",
    "    \n",
    "    # restore old variables\n",
    "    MINIMAX_DEPTH_LIMIT = old_mm_depth\n",
    "    ALPHABETA_DEPTH_LIMIT = old_ab_depth\n",
    "    PROBCUT_DEPTH_LIMIT = old_pb_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_num_visited_states(test_board, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `get_statistics` ist dazu da, mehrere Spiele zu berechnen, in denen zwei KIs gegeneinander spielen und Statistiken darüber zu sammeln. `num` ist die Anzahl der Spiele, die durchgeführt werden sollen. Die weiteren Parameter legen fest, welche KIs und welche Heuristiken für die Spieler verwendet werden sollen. Statistiken werden nach jedem Spiel aktualisiert.\n",
    "\n",
    "Da das Spielfeld nicht gezeichnet werden soll, werden statt `next_move` die Funktion `next_move_blind` verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(num, black_ai, black_h, white_ai, white_h):\n",
    "    status = ipywidgets.widgets.Label()\n",
    "    display(status)\n",
    "    result = []\n",
    "    wins = [0, 0, 0]\n",
    "    status.value = f'0 / {num} games played, b/d/w: {wins[0]}/{wins[1]}/{wins[2]}'\n",
    "    try:\n",
    "        for i in range(num):\n",
    "            (b, w) = play_game(black_ai, black_h, white_ai, white_h)\n",
    "            result.append((b, w))\n",
    "            if b > w:\n",
    "                wins[0] += 1\n",
    "            elif w == b:\n",
    "                wins[1] += 1\n",
    "            else:\n",
    "                wins[2] += 1\n",
    "            status.value = f'{i+1} / {num} games played, b/d/w: {wins[0]}/{wins[1]}/{wins[2]}'\n",
    "    except KeyboardInterrupt:\n",
    "        status.value = f'Interrupted: {i} / {num} games played, b/d/w: {wins[0]}/{wins[1]}/{wins[2]}'\n",
    "    print_statistics(result)\n",
    "\n",
    "def play_game(black_ai, black_h, white_ai, white_h):\n",
    "    state = GameState()\n",
    "    next_move_blind(state, black_ai, white_ai, {BLACK: black_h, WHITE: white_h})\n",
    "    return count_disks(state, BLACK), count_disks(state, WHITE)\n",
    "\n",
    "\n",
    "def next_move_blind(state, black_ai, white_ai, heuristics):\n",
    "    # Check if/which AI is playing\n",
    "    strat = black_ai if state.turn == BLACK else white_ai\n",
    "    strat(state, heuristics[state.turn])\n",
    "    if not state.game_over:\n",
    "        next_move_blind(state, black_ai, white_ai, heuristics)\n",
    "\n",
    "def print_statistics(results):\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_statistics(5, random_ai_make_move, cowthello_heuristic, random_ai_make_move, cowthello_heuristic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
